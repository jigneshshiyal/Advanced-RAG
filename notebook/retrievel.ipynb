{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3179733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine simalarity search and mmr, TFIDF or BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ad1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_MODEL = os.getenv(\"GEMINI_MODEL\", \"models/gemini-2.5-flash-lite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f68a8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jignesh.shiyal\\Documents\\rag\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=GEMINI_MODEL, api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "743be808",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is deepseek model doing?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f13662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./store_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08961e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "938ed855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "db = Chroma(collection_name=\"rag_collection\", client=client, embedding_function=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f52d879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_results = db._collection.query(\n",
    "        query_embeddings=embeddings.embed_query(query),\n",
    "        n_results=20,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "26b42d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "sim_docs = [\n",
    "        Document(page_content=d, metadata=m)\n",
    "        for d, m in zip(sim_results[\"documents\"][0], sim_results[\"metadatas\"][0])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16a880fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":10, \"fetch_k\":20, \"lambda_mult\":0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b8f67c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_docs = mmr_retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63a026b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='21af96dc89d8ee66129759b8e11549dd', metadata={'page_number': 2, 'source': '../data/DeepSeek-R1.pdf'}, page_content='Introduction Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . . Reinforcement Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Training Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . . Cold Start . . . . . . . . . . . . . . . .'),\n",
       " Document(id='5ed9c492191c6a5986ec8cfe9af2ff9b', metadata={'source': '../data/DeepSeek-R1.pdf', 'page_number': 1}, page_content='DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning DeepSeek-AI research@deepseek.com We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super- vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek- R1 achieves performance comparable to'),\n",
       " Document(id='3e196bbb81c048fdcae0706f848b6188', metadata={'page_number': 4, 'source': '../data/DeepSeek-R1.pdf'}, page_content='and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks.   Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek- R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks.'),\n",
       " Document(id='3fe0213f2c26926fcd51974a71c46f2d', metadata={'page_number': 4, 'source': '../data/DeepSeek-R1.pdf'}, page_content='Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.'),\n",
       " Document(id='89a371c87bee2f2accbe6a0dace7832b', metadata={'source': '../data/DeepSeek-R1.pdf', 'page_number': 16}, page_content='DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency.'),\n",
       " Document(id='d1ca76bea4802ceadd7c71fca3008207', metadata={'page_number': 9, 'source': '../data/DeepSeek-R1.pdf'}, page_content='version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. 2.3. DeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1)'),\n",
       " Document(id='bd0bfa02e946fbd7db1ad53eae883ffc', metadata={'source': '../data/DeepSeek-R1.pdf', 'page_number': 16}, page_content='we plan to invest in research across the following directions for DeepSeek-R1.   General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.   Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future   Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts.'),\n",
       " Document(id='48942d34d4be2cf7bef626e6a153ea91', metadata={'page_number': 6, 'source': '../data/DeepSeek-R1.pdf'}, page_content='train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:   Accuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.   Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between   and   We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that'),\n",
       " Document(id='c10b68ca3ca3f52b11c285ce3e8fcb2d', metadata={'source': '../data/DeepSeek-R1.pdf', 'page_number': 13}, page_content='improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1 s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that'),\n",
       " Document(id='3a05b056206f688782becfc9e50c3099', metadata={'page_number': 14, 'source': '../data/DeepSeek-R1.pdf'}, page_content='Distillation v.s. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "822fdb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "377a72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [doc.page_content for doc in sim_docs]\n",
    "tokenized_corpus = [c.split() for c in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "bm25_scores = bm25.get_scores(query.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "50330c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "alpha=0.6\n",
    "weight_mmr=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fb823048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def normalize(arr):\n",
    "    arr = np.array(arr, dtype=float)\n",
    "    lo, hi = arr.min(), arr.max()\n",
    "    if hi - lo < 1e-8:\n",
    "        return np.ones_like(arr) * 0.5\n",
    "    return (arr - lo) / (hi - lo)\n",
    "\n",
    "# --- Hybrid retrieval (similarity + MMR + BM25) ---\n",
    "def hybrid_retrieve(query, chroma_db, embeddings, k=10, alpha=0.6, weight_mmr=0.3):\n",
    "    \"\"\"\n",
    "    Combines similarity, MMR, and BM25 retrieval into a normalized hybrid score.\n",
    "    alpha: weight for semantic similarity\n",
    "    weight_mmr: weight for MMR diversity\n",
    "    (1 - alpha - weight_mmr): weight for BM25 lexical relevance\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: similarity docs\n",
    "    sim_results = chroma_db._collection.query(\n",
    "        query_embeddings=embeddings.embed_query(query),\n",
    "        n_results=20,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    sim_docs = [\n",
    "        Document(page_content=d, metadata=m)\n",
    "        for d, m in zip(sim_results[\"documents\"][0], sim_results[\"metadatas\"][0])\n",
    "    ]\n",
    "    sim_dists = np.array(sim_results[\"distances\"][0])\n",
    "    sim_scores = 1 / (1 + sim_dists)  # convert distance â†’ similarity\n",
    "    sim_norm = normalize(sim_scores)\n",
    "\n",
    "    # Step 2: mmr docs\n",
    "    mmr_docs = chroma_db.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 10, \"fetch_k\": 20, \"lambda_mult\": 0.5}\n",
    "    ).invoke(query)\n",
    "\n",
    "    # Step 3: BM25\n",
    "    corpus = [doc.page_content for doc in sim_docs]\n",
    "    tokenized_corpus = [c.split() for c in corpus]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    bm25_scores = normalize(bm25.get_scores(query.split()))\n",
    "\n",
    "    # Step 4: combine scores using stable doc IDs\n",
    "    doc_scores = defaultdict(float)\n",
    "    doc_id_map = {id(doc): doc for doc in sim_docs}\n",
    "\n",
    "    # semantic similarity\n",
    "    for i, doc in enumerate(sim_docs):\n",
    "        doc_scores[id(doc)] += alpha * sim_norm[i]\n",
    "\n",
    "    # MMR reciprocal-rank\n",
    "    for i, doc in enumerate(mmr_docs):\n",
    "        key = id(doc)\n",
    "        doc_scores[key] += weight_mmr / (i + 1)\n",
    "\n",
    "    # BM25 lexical\n",
    "    for i, score in enumerate(bm25_scores):\n",
    "        doc_scores[id(sim_docs[i])] += (1 - alpha - weight_mmr) * score\n",
    "\n",
    "    # Step 5: rank and attach metadata\n",
    "    ranked = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    ranked_docs = []\n",
    "    for key, score in ranked:\n",
    "        doc = doc_id_map.get(key)\n",
    "        if doc:\n",
    "            doc.metadata[\"hybrid_score\"] = float(score)\n",
    "            ranked_docs.append(doc)\n",
    "\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45ddea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_docs = hybrid_retrieve(query, db, embeddings, k=10, alpha=0.6, weight_mmr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "49843d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/DeepSeek-R1.pdf', 'page_number': 2, 'hybrid_score': 0.6}, page_content='Introduction Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . . Reinforcement Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Training Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . . Cold Start . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'page_number': 1, 'source': '../data/DeepSeek-R1.pdf', 'hybrid_score': 0.4617633824754085}, page_content='DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning DeepSeek-AI research@deepseek.com We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super- vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek- R1 achieves performance comparable to'),\n",
       " Document(metadata={'source': '../data/DeepSeek-R1.pdf', 'page_number': 4, 'hybrid_score': 0.26491243630891853}, page_content='1.1. Contributions Post-Training: Large-Scale Reinforcement Learning on the Base Model   We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek- R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.   We introduce our pipeline to develop DeepSeek-R1. The pipeline'),\n",
       " Document(metadata={'source': '../data/DeepSeek-R1.pdf', 'page_number': 13, 'hybrid_score': 0.24981658468446616}, page_content='between DeepSeek-R1 and other representative models. For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im- provement is primarily attributed to enhanced accuracy in STEM-related questions, where signif- icant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses'),\n",
       " Document(metadata={'source': '../data/DeepSeek-R1.pdf', 'page_number': 4, 'hybrid_score': 0.19799466648790193}, page_content='and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks.   Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek- R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks.'),\n",
       " Document(metadata={'source': '../data/DeepSeek-R1.pdf', 'page_number': 4, 'hybrid_score': 0.18388642703915714}, page_content='Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.'),\n",
       " Document(metadata={'page_number': 3, 'source': '../data/DeepSeek-R1.pdf', 'hybrid_score': 0.1696441319652016}, page_content='et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of'),\n",
       " Document(metadata={'page_number': 3, 'source': '../data/DeepSeek-R1.pdf', 'hybrid_score': 0.16264698656888557}, page_content='enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1- Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ca82814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "cross_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")  # precise reranker\n",
    "\n",
    "def rerank_with_cross_encoder(query, candidates, weight_cross=0.7, weight_hybrid=0.3):\n",
    "    \"\"\"\n",
    "    Re-rank top documents using cross-encoder.\n",
    "    Combines normalized cross-encoder scores and hybrid retrieval scores.\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return []\n",
    "\n",
    "    pairs = [[query, c.page_content] for c in candidates]\n",
    "    cross_scores = cross_model.predict(pairs)\n",
    "    cross_norm = softmax(cross_scores)\n",
    "\n",
    "    hybrid_arr = np.array([c.metadata.get(\"hybrid_score\", 0.0) for c in candidates])\n",
    "    hybrid_norm = normalize(hybrid_arr)\n",
    "\n",
    "    combined = weight_cross * cross_norm + weight_hybrid * hybrid_norm\n",
    "\n",
    "    for c, cs, cn, hn, final in zip(\n",
    "        candidates, cross_scores, cross_norm, hybrid_norm, combined\n",
    "    ):\n",
    "        c.metadata[\"cross_score\"] = float(cs)\n",
    "        c.metadata[\"cross_norm\"] = float(cn)\n",
    "        c.metadata[\"hybrid_norm\"] = float(hn)\n",
    "        c.metadata[\"final_score\"] = float(final)\n",
    "\n",
    "    ranked = sorted(candidates, key=lambda x: x.metadata[\"final_score\"], reverse=True)\n",
    "    return ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3cd34ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_score = rerank_with_cross_encoder(query, ranked_docs, weight_cross=0.7, weight_hybrid=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "13d490a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page_number': 1, 'source': '../data/DeepSeek-R1.pdf', 'hybrid_score': 0.4617633824754085, 'cross_score': 5.093067646026611, 'cross_norm': 0.2590062618255615, 'hybrid_norm': 0.6839243968159727, 'final_score': 0.38648169934245263}, page_content='DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning DeepSeek-AI research@deepseek.com We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super- vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek- R1 achieves performance comparable to'),\n",
       " Document(metadata={'source': '../data/DeepSeek-R1.pdf', 'page_number': 2, 'hybrid_score': 0.6, 'cross_score': 0.8823403716087341, 'cross_norm': 0.0038425070233643055, 'hybrid_norm': 1.0, 'final_score': 0.30268975482322275}, page_content='Introduction Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . . Reinforcement Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Training Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . . Cold Start . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': '../data/DeepSeek-R1.pdf', 'page_number': 4, 'hybrid_score': 0.26491243630891853, 'cross_score': 4.269309043884277, 'cross_norm': 0.11364659667015076, 'hybrid_norm': 0.2338281584885898, 'final_score': 0.14970106074533412}, page_content='1.1. Contributions Post-Training: Large-Scale Reinforcement Learning on the Base Model   We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek- R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.   We introduce our pipeline to develop DeepSeek-R1. The pipeline'),\n",
       " Document(metadata={'source': '../data/DeepSeek-R1.pdf', 'page_number': 13, 'hybrid_score': 0.24981658468446616, 'cross_score': 4.290770530700684, 'cross_norm': 0.11611198633909225, 'hybrid_norm': 0.19931175832474352, 'final_score': 0.1410719186798457}, page_content='between DeepSeek-R1 and other representative models. For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im- provement is primarily attributed to enhanced accuracy in STEM-related questions, where signif- icant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses'),\n",
       " Document(metadata={'source': '../data/DeepSeek-R1.pdf', 'page_number': 4, 'hybrid_score': 0.19799466648790193, 'cross_score': 4.560911178588867, 'cross_norm': 0.15212397277355194, 'hybrid_norm': 0.08082185061835366, 'final_score': 0.13073333761710856}, page_content='and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks.   Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek- R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks.'),\n",
       " Document(metadata={'page_number': 3, 'source': '../data/DeepSeek-R1.pdf', 'hybrid_score': 0.16264698656888557, 'cross_score': 4.765894889831543, 'cross_norm': 0.1867329329252243, 'hybrid_norm': 0.0, 'final_score': 0.13071304559707642}, page_content='enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1- Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as'),\n",
       " Document(metadata={'source': '../data/DeepSeek-R1.pdf', 'page_number': 4, 'hybrid_score': 0.18388642703915714, 'cross_score': 3.9937894344329834, 'cross_norm': 0.08627794682979584, 'hybrid_norm': 0.048563608385007506, 'final_score': 0.07496364529635934}, page_content='Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.'),\n",
       " Document(metadata={'page_number': 3, 'source': '../data/DeepSeek-R1.pdf', 'hybrid_score': 0.1696441319652016, 'cross_score': 3.946073293685913, 'cross_norm': 0.08225777745246887, 'hybrid_norm': 0.01599885031412527, 'final_score': 0.06238010005602385}, page_content='et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480b804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a59a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6575356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab27a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9c9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ccee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac532f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
