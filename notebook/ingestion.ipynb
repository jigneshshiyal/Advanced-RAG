{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "552c72c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd682971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/data science learning - LLM training and inference.pdf',\n",
       " '../data/DeepSeek-R1.pdf']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "all_pdfs = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "\n",
    "all_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed193543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    \n",
    "    # Only use BeautifulSoup if text looks like HTML\n",
    "    if bool(re.search(r\"<[^>]+>\", text)):\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Optional: remove non-ASCII\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_text(pdf_path, remove_headers_footers=True):\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    pages_text = []\n",
    "    header_footer_candidates = []\n",
    "\n",
    "    for page in doc:\n",
    "        page_text = page.get_text(\"text\") or \"\"\n",
    "        lines = [clean_text(line.strip()) for line in page_text.split(\"\\n\") if len(line.strip()) > 10]\n",
    "        if remove_headers_footers:\n",
    "            header_footer_candidates.extend(lines[:2] + lines[-2:])\n",
    "        pages_text.append((page.number + 1, lines))\n",
    "\n",
    "    repetitive = set()\n",
    "    if remove_headers_footers:\n",
    "        counts = Counter(header_footer_candidates)\n",
    "        repetitive = {line for line, c in counts.items() if c > 0.6 * len(pages_text)}\n",
    "\n",
    "    text_docs = []\n",
    "    for page_num, lines in pages_text:\n",
    "        clean_lines = [ln for ln in lines if ln not in repetitive]\n",
    "        text_docs.append({\n",
    "            \"metadata\": {\"page_number\": page_num, \"source\": pdf_path},\n",
    "            \"text\": \" \".join(clean_lines)\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "    return text_docs\n",
    "\n",
    "\n",
    "def pdfs_loader(paths):\n",
    "    all_docs = []\n",
    "    for pdf_path in paths:\n",
    "        docs = extract_text(pdf_path)\n",
    "        all_docs.extend(docs)\n",
    "    return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "30f7498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pdfs_loader(all_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1099b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'metadata': {'page_number': 1,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': 'data science learning - LLM training and inference i want to learn ,how to large language model work , how to train or fine-tune and how to make efficient model inference , internel architecture and any other components Great   let s build a structured roadmap so you can deeply understand how large language models (LLMs) work, how they are trained/fine-tuned, and how inference is optimized. I ll break this into digestible parts, from foundations to advanced topics. 1. Foundations of LLMs Before diving into training and optimization, you need to be comfortable with the core building blocks. Neural Networks Basics Perceptron   Multi-Layer Perceptron (MLP) Backpropagation & Gradient Descent Sequence Models Evolution RNNs, LSTMs, GRUs   limitations (vanishing gradients, poor long- range memory) Self-Attention introduced in  Attention is All You Need  (2017) Transformers Encoder, Decoder, Encoder-Decoder Self-Attention mechanism: Query, Key, Value Positional Encodings Multi-Head Attention Feed-Forward layers, Residual Connections, Layer Norm'},\n",
       " {'metadata': {'page_number': 2,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': '  Action for you: Try to implement a tiny Transformer from scratch in PyTorch or TensorFlow. 2. Pretraining of LLMs LLMs like GPT, LLaMA, Falcon, Mistral are trained in two main stages. Pretraining Objective Usually causal language modeling (CLM)   predict next token. On massive datasets (hundreds of billions of tokens). Training Pipeline Tokenization (BPE, SentencePiece, WordPiece) Datasets (Common Crawl, books, code, Wikipedia, etc.) Loss Function: Cross-Entropy Optimization: AdamW, learning rate warm-up, cosine decay Mixed Precision Training (FP16/BF16) Gradient Accumulation, Gradient Clipping Distributed Training: Data Parallelism, Model Parallelism, Pipeline Parallelism   Deep dive topics: scaling laws, compute-data tradeoff, Chinchilla optimal 3. Fine-Tuning LLMs After pretraining, models are specialized. Full Fine-Tuning Update all model parameters (costly for big LLMs). Parameter-Efficient Fine-Tuning (PEFT) LoRA (Low-Rank Adaptation) Prefix Tuning, P-Tuning, Adapters'},\n",
       " {'metadata': {'page_number': 3,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': 'QLoRA (quantized LoRA) for resource-constrained training Instruction Tuning Supervised Fine-Tuning (SFT) on curated instruction-response pairs Reinforcement Learning with Human Feedback (RLHF) Reward model from human preferences Proximal Policy Optimization (PPO) or DPO (Direct Preference Optimization)   Action for you: Try fine-tuning a small LLaMA/Mistral model using Hugging Face peft and trl libraries. 4. Inference Optimization (Efficient Serving) Running LLMs in production is another challenge. Quantization FP32   FP16   INT8   INT4 (trade-off between accuracy and speed/memory) Pruning & Distillation Remove redundant weights or distill into smaller models Batching & KV-Caching Reuse attention key-value states for faster autoregressive decoding Efficient batching across users Serving Frameworks vLLM, TensorRT-LLM, DeepSpeed-Inference, Hugging Face text- generation-inference Streaming & Speculative Decoding Stream tokens while computing Use smaller draft models to speed up larger ones   Action for you: Experiment with quantized LLaMA 2/3 using bitsandbytes or auto-gptq and measure inference speed vs accuracy.'},\n",
       " {'metadata': {'page_number': 4,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': '5. Internal Architecture & Components When people say  how LLMs work internally,  they usually mean: Embedding Layer   converts tokens to vectors. Stack of Transformer Blocks   attention + feedforward layers. Layer Norm & Residuals   stabilize training. Output Layer (LM Head)   predicts token logits. Parameter Count vs Context Length   scaling considerations. 6. Advanced Topics Mixture of Experts (MoE)   only activate subset of weights per token. Retrieval-Augmented Generation (RAG)   use external vector DB for Memory & Context Window Extensions   FlashAttention, RoPE, ALiBi, sliding window attention. Evaluation Metrics   perplexity, BLEU, MMLU, HELM, MT-Bench.   Suggested Learning Path: 1. Learn Transformers basics   build toy model. 2. Explore pretraining pipeline   try pretraining on small dataset. 3. Learn fine-tuning (LoRA/QLoRA, SFT, RLHF)   practice with open-source 4. Dive into inference optimization   experiment with quantization & 5. Explore advanced architectures (MoE, RAG, long context).'},\n",
       " {'metadata': {'page_number': 5,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': 'Would you like me to make a step-by-step curriculum (with resources, papers, and small coding projects at each stage), so you can follow along like a structured learning plan? Perfect     let s build you a step-by-step curriculum for learning how large language models (LLMs) work, how to train/fine-tune them, and how to make inference efficient. I ll mix theory, papers, and coding projects so you get both intuition and hands-on skills.   Step-by-Step Curriculum for LLMs Stage 1: Core Foundations Goal: Build intuition about neural nets, attention, and Transformers. Concepts to Learn Neural nets basics (MLPs, backpropagation, optimization) Word embeddings & tokenization RNNs   LSTMs   Attention mechanism Transformer architecture Key Resources The Illustrated Transformer (visual explanation) Attention Is All You Need (2017)   original Transformer paper Stanford CS324: Large Language Models (lectures on YouTube) Coding Projects Implement a tiny Transformer (2 3 layers) in PyTorch.'},\n",
       " {'metadata': {'page_number': 6,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': 'Train it on a toy dataset (e.g., character-level text generation on Shakespeare). Stage 2: Pretraining Pipeline Goal: Understand how GPT-like models are trained. Concepts to Learn Causal language modeling (CLM) objective Tokenization (BPE, SentencePiece) Training loops: batching, loss (cross-entropy), optimizers (AdamW) Scaling laws (Chinchilla, compute-data tradeoff) Distributed training (data parallelism, model parallelism) Key Resources Hugging Face Course: Transformers Scaling Laws for Neural Language Models (Kaplan et al., 2020) Training Compute-Optimal Large Language Models (DeepMind Chinchilla, 2022) Coding Projects Tokenize and pretrain a GPT-2-sized model on a small dataset (WikiText-2). Use Hugging Face transformers + datasets . Stage 3: Fine-Tuning & Adaptation Goal: Learn how to make pretrained models useful for tasks. Concepts to Learn Full fine-tuning vs parameter-efficient fine-tuning (PEFT) LoRA, QLoRA, adapters, prefix tuning Instruction tuning with curated datasets'},\n",
       " {'metadata': {'page_number': 7,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': 'RLHF (reinforcement learning with human feedback): PPO, DPO Key Resources LoRA: Low-Rank Adaptation of LLMs (Hu et al., 2021) Hugging Face PEFT library docs InstructGPT paper (OpenAI, 2022) Coding Projects Fine-tune a small model (LLaMA-7B or Mistral-7B) on a custom dataset with LoRA. Try QLoRA for GPU memory efficiency. Run supervised fine-tuning (SFT) on Alpaca dataset. Stage 4: Inference & Deployment Goal: Make LLMs run fast and cheap in production. Concepts to Learn Quantization (FP16   INT8   INT4) KV-caching for autoregressive decoding Efficient batching & streaming Speculative decoding (small model drafts for big models) Serving frameworks: vLLM, DeepSpeed-Inference, TGI Key Resources vLLM docs (vllm.ai Hugging Face Text Generation Inference Accelerating Inference with Speculative Decoding (Leviathan et al., 2023) Coding Projects Quantize a model with bitsandbytes or auto-gptq . Serve a model with vLLM and benchmark throughput. Implement streaming token generation.'},\n",
       " {'metadata': {'page_number': 8,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': 'Stage 5: Advanced Architectures & Research Goal: Explore cutting-edge improvements. Concepts to Learn Mixture of Experts (MoE) models (Switch Transformer, Mixtral) Long-context attention (RoPE, ALiBi, FlashAttention) Retrieval-Augmented Generation (RAG) Evaluation frameworks: MMLU, HELM, MT-Bench Key Resources Switch Transformers (Fedus et al., 2021) FlashAttention (Dao et al., 2022) Retrieval-Augmented Generation (Lewis et al., 2020) Stanford HELM benchmark Coding Projects Add retrieval (RAG) to an open-source LLM using FAISS/Chroma DB. Experiment with long-context models (Mistral-Long, LLaMA-Long). Fine-tune a reward model for preferences (mini-RLHF experiment). Stage 6: Putting It All Together Goal: Build an end-to-end mini-LLM project. Choose a domain (e.g., finance, medical Q&A, education). Pretrain a small model on a domain dataset. Fine-tune with LoRA for instruction following. Optimize inference with quantization + caching. Add RAG for domain-specific knowledge. Deploy via API.'},\n",
       " {'metadata': {'page_number': 9,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': '  By the end, you ll understand internal architecture, training, fine-tuning, inference efficiency, and advanced methods. Do you want me to turn this curriculum into a 12-week learning plan (weekly breakdown with reading + coding tasks), so you can follow it like a structured course? explain basic term from NN, LLM and quantazation , explain Absolutely   let s start by explaining the basic terms you ll encounter in Neural Networks (NNs), Large Language Models (LLMs), and Quantization   in a clear, intuitive way with short examples.   1. Basic Terms from Neural Networks (NN) These are the building blocks behind all deep learning, including LLMs. A neuron takes inputs, multiplies them by weights, adds a bias, and applies an activation function. Think of it as a single  decision unit . output = activation(w1*x1 + w2*x2 + b)'},\n",
       " {'metadata': {'page_number': 10,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': 'A collection of neurons stacked together. Input layer: Takes raw data (like words, images, etc.) Hidden layers: Learn patterns Output layer: Produces final prediction 1.3 Weights and Biases Weights (w): Control the strength of connections between neurons. Bias (b): Allows shifting the activation threshold. These are the parameters the network learns during training. 1.4 Activation Function Adds non-linearity, so the network can learn complex patterns. Common types: ReLU (Rectified Linear Unit): max(0, x) Sigmoid: 1 / (1 + e^-x) Tanh: outputs between -1 and 1 1.5 Loss Function Measures how far the model s predictions are from the correct answers. Example for classification: Cross-Entropy Loss 1.6 Backpropagation Algorithm to adjust weights using gradients.'},\n",
       " {'metadata': {'page_number': 11,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': 'It calculates how changing each weight will affect the loss, then updates the weights to minimize the error. 1.7 Optimizer Controls how weights are updated. Common optimizers: SGD (Stochastic Gradient Descent) Adam / AdamW (used in LLMs)   2. Basic Terms from Large Language Models LLMs are specialized neural networks (Transformers) trained on text to predict the next word or token. A piece of text (can be a word, subword, or character).  Playing    [\"Play\", \"ing\"] Tokenization converts text into numerical IDs so the model can process 2.2 Embedding A vector representation of a token.'},\n",
       " {'metadata': {'page_number': 12,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': 'The word  king  might be represented as [0.23, -0.41, 0.87, ...] Similar words have nearby embeddings. 2.3 Transformer The core architecture behind LLMs. Built from self-attention layers and feed-forward networks. 2.4 Attention Mechanism Allows the model to  focus  on relevant parts of the input sequence. In the sentence  The cat sat on the mat,  when predicting  mat,  the model attends to  sat  and  on . 2.5 Self-Attention (Q, K, V) Each token gets: Query (Q): what it s looking for Key (K): what it offers Value (V): information it provides The model computes attention weights between Q and K to decide how much each token should influence the others. 2.6 Positional Encoding Since Transformers don t process data sequentially, positional encodings tell the model the order of tokens in the sentence.'},\n",
       " {'metadata': {'page_number': 13,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': '2.7 Feed-Forward Network (FFN) A small neural network inside each Transformer block that processes token representations after attention. 2.8 Layer Normalization & Residuals LayerNorm: stabilizes training by normalizing activations. Residual connections: help preserve information by adding inputs back to outputs ( x + f(x) ). 2.9 LM Head (Language Modeling Head) Final layer that converts hidden vectors into probabilities over vocabulary (predicting the next token). 2.10 Context Window The number of tokens the model can  see  at once. GPT-3: 2048 tokens GPT-4: up to 128K or more   3. Basic Terms from Quantization Quantization is about making models smaller and faster for inference. 3.1 Precision Refers to how many bits are used to store a number. FP32 = 32-bit floating point'},\n",
       " {'metadata': {'page_number': 14,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': 'FP16 = 16-bit INT8 / INT4 = 8-bit or 4-bit integer Smaller precision   less memory, faster computation (but may reduce 3.2 Quantization The process of converting model weights (and sometimes activations) from high precision (FP32) to lower precision (INT8, INT4). 3.3 Types of Quantization Post-Training Quantization (PTQ): Quantize after training. Quantization-Aware Training (QAT): Simulate quantization during training for better accuracy. 3.4 Dynamic vs Static Quantization Dynamic: Quantize weights only during inference. Static: Quantize both weights and activations ahead of time. FP32 weight: 0.324582934 Quantized INT8: 0.32'},\n",
       " {'metadata': {'page_number': 15,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': 'Combines quantization and LoRA (Low-Rank Adaptation). Allows fine-tuning large models (like LLaMA 70B) efficiently on small 3.6 Trade-Off   Small drop   Very fast   Very small   More loss 3.7 Quantized Inference Using quantized weights in production to serve models efficiently. Libraries: bitsandbytes , auto-gptq , ggml , exllama , vLLM .   In short: Neural Networks = how learning happens LLMs = what structure learns text Quantization = how to make models faster and smaller Would you like me to now draw a visual map (like a diagram explaining how all these components fit together inside an LLM   from input tokens  '},\n",
       " {'metadata': {'page_number': 16,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': 'embeddings   transformer layers   quantized output)? explain lora in simple word Absolutely     let s explain LoRA (Low-Rank Adaptation) in very simple terms, step by step.   What Is LoRA? LoRA stands for Low-Rank Adaptation   it s a method to fine-tune large language models (LLMs) without changing most of their weights. It was introduced to make fine-tuning big models cheap and efficient  .   Imagine This Scenario You have a huge model like LLaMA 7B (7 billion parameters). Fine-tuning all its parameters would: Need lots of GPU memory Take days or weeks Risk forgetting what it already knows (catastrophic forgetting) So instead of updating all weights, LoRA says:    Let s keep the original model frozen and just learn some small extra   How It Works (Simplified)'},\n",
       " {'metadata': {'page_number': 17,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': 'Every Transformer layer has big matrices   for example, W   that help it process information. Normally, fine-tuning means updating this entire matrix (maybe millions of So LoRA replaces: A and B are small matrices (much smaller than W). Only A and B are trained. W_original is frozen (unchanged).   Why  Low-Rank ? The term low-rank just means that A   B is a small adjustment   it has much fewer parameters than W. If W is size 1000 1000 (1,000,000 parameters), LoRA might make A (1000 4) and B (4 1000), which is only 8,000 parameters   125  smaller!   So What Happens in Practice  We don t need to change all these numbers   we can just learn a small correction.  W_new = W_original + (A   B)'},\n",
       " {'metadata': {'page_number': 18,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': '1. You load a pretrained model (e.g., LLaMA, Mistral). 2. Freeze all weights so they don t change. 3. Insert LoRA adapters (the small A and B matrices). 4. Train only the adapters on your new dataset (like chat, finance, etc.). 5. When generating text, the model uses both the base weights and LoRA s learned updates.   Benefits of LoRA   Few trainable Only train a small part   saves You can load multiple LoRAs for different tasks Fine-tune huge models on a   Mergeable LoRA weights can be merged back into the base model later   Example in Hugging Face (Python) from peft import LoraConfig, get_peft_model from transformers import AutoModelForCausalLM # Load base model model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")'},\n",
       " {'metadata': {'page_number': 19,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'text': 'This modifies the model without touching the base weights. Think of LoRA like putting sticky notes on top of a textbook. The textbook = pretrained model (frozen) The sticky notes = LoRA adapters (new info) You re adding knowledge without rewriting the whole book!   In short: Would you like me to explain QLoRA next (it s the  quantized  version of LoRA that makes it even faster and lighter)? # Define LoRA settings config = LoraConfig( r=8, # rank (how small the adapter is) lora_alpha=32, # scaling factor target_modules=[\"q_proj\", \"v_proj\"], # layers to inject LoRA into lora_dropout=0.05 # Add LoRA adapters model = get_peft_model(model, config) LoRA lets you fine-tune giant models by training just a few extra, small matrices   it s cheap, efficient, and modular.'},\n",
       " {'metadata': {'page_number': 1, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning DeepSeek-AI research@deepseek.com We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super- vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek- R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama. (Percentile) GPQA Diamond SWE-bench Verified Accuracy / Percentile (%) DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3 Figure 1 | Benchmark performance of DeepSeek-R1. arXiv:2501.12948v1 [cs.CL] 22 Jan 2025'},\n",
       " {'metadata': {'page_number': 2, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'Introduction Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . . Reinforcement Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Training Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . . Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . . Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . . Reinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . . . . Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . . DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Distilled Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Conclusion, Limitations, and Future Work A Contributions and Acknowledgments'},\n",
       " {'metadata': {'page_number': 3, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': '1. Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI s o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of- Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI s o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1- Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5- 32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru- cial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models.'},\n",
       " {'metadata': {'page_number': 4, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': '1.1. Contributions Post-Training: Large-Scale Reinforcement Learning on the Base Model   We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek- R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human pref- erences, as well as two SFT stages that serve as the seed for the model s reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. Distillation: Smaller Models Can Be Powerful Too   We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.   Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek- R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi- tionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open- source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. 1.2. Summary of Evaluation Results   Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-o1-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks.   Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek- R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.'},\n",
       " {'metadata': {'page_number': 5, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': '  Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are- naHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. 2. Approach 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. 2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev- idenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. 2.2.1. Reinforcement Learning Algorithm Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question q, GRPO samples a group of outputs {o1, o2,       , oG} from the old policy  oldand then optimizes the policy model  by maximizing the following objective: JGRPO( ) = E[q P(Q), {oi}G i=1  old(O|q)]  old(oi|q) Ai, clip  old(oi|q) , 1  , 1 +    (oi|q)  log  (oi|q)  1, where  and  are hyper-parameters, and Aiis the advantage, computed using a group of rewards {r1, r2, . . . , rG} corresponding to the outputs within each group: Ai= ri mean({r1, r2,       , rG}) std({r1, r2,       , rG})'},\n",
       " {'metadata': {'page_number': 6, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within and tags, respectively, i.e., reasoning process here answer here . User: prompt. Assistant: Table 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning question during training. 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:   Accuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.   Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between   and   We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biases such as mandating reflective reasoning or promoting particular problem-solving strate- gies to ensure that we can accurately observe the model s natural progression during the RL 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek- R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the model s performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI s o1-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers'},\n",
       " {'metadata': {'page_number': 7, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'OpenAI-o1-mini OpenAI-o1-0912 DeepSeek-R1-Zero Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the model s ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek- R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero s performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the model s progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-'},\n",
       " {'metadata': {'page_number': 8, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. ment throughout the training process. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time compu- tation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflection where the model revisits and reevaluates its previous steps and the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the model s interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zero s reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an  aha moment . This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the model s growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an  aha moment  for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The  aha moment  serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.'},\n",
       " {'metadata': {'page_number': 9, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'Question: If a> 1, then the sum of the real solutions of a+ x= xis equal to Response: To solve the equation a+ x= x, let s start by squaring both       Rearrange to isolate the inner square root term: (a x2)2 = a+ x= a2  2ax2 + (x2)2 = a+ x= x4  2ax2  x+ (a2  a) = 0 Wait, wait. Wait. That s an aha moment I can flag here. Let s reevaluate this step-by-step to identify if the correct sum can be       We started with the equation: First, let s square both sides: a+ x= x2 =  Next, I could square both sides again, treating the equation:       Table 3 | An interesting  aha moment  of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. 2.3. DeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. 2.3.1. Cold Start Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1- Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data'},\n",
       " {'metadata': {'page_number': 10, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': '  Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as |special_token||special_token|, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning   Potential: By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the model s reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the model s performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model s capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. Reasoning data We curate reasoning prompts and generate reasoning trajectories by perform- ing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples.'},\n",
       " {'metadata': {'page_number': 11, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as  hello  we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model s helpfulness and harmlessness while simultane- ously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train- ing prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in  2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5- 14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. 3. Experiment We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,'},\n",
       " {'metadata': {'page_number': 12, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': '2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08   2025-01), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math- ematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple- evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor- mance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@kevaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-pvalue of 0.95 to generate k responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as where pidenotes the correctness of the i-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. 1https://aider.chat 2https://codeforces.com 3https://www.cms.org.cn/Home/comp/comp/cid/12.html'},\n",
       " {'metadata': {'page_number': 13, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': '3.1. DeepSeek-R1 Evaluation Benchmark (Metric) Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek Sonnet-1022 o1-mini o1-1217 Architecture # Activated Params # Total Params MMLU (Pass@1) MMLU-Redux (EM) MMLU-Pro (EM) DROP (3-shot F1) IF-Eval (Prompt Strict) GPQA Diamond (Pass@1) SimpleQA (Correct) FRAMES (Acc.) AlpacaEval2.0 (LC-winrate) ArenaHard (GPT-4-1106) LiveCodeBench (Pass@1-COT) Codeforces (Percentile) Codeforces (Rating) SWE Verified (Resolved) Aider-Polyglot (Acc.) AIME 2024 (Pass@1) MATH-500 (Pass@1) CNMO 2024 (Pass@1) CLUEWSC (EM) C-Eval (EM) C-SimpleQA (Correct) Table 4 | Comparison between DeepSeek-R1 and other representative models. For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im- provement is primarily attributed to enhanced accuracy in STEM-related questions, where signif- icant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a model s ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1 s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that'},\n",
       " {'metadata': {'page_number': 14, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. 3.2. Distilled Model Evaluation GPT-4o-0513 Claude-3.5-Sonnet-1022 OpenAI-o1-mini QwQ-32B-Preview DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-14B DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Llama-70B Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. As shown in Table 5, simply distilling DeepSeek-R1 s outputs enables the efficient DeepSeek- R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non- reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B- Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla- tion. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. 4. Discussion 4.1. Distillation v.s. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale'},\n",
       " {'metadata': {'page_number': 15, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'GPQA Diamond LiveCodeBench QwQ-32B-Preview DeepSeek-R1-Zero-Qwen-32B DeepSeek-R1-Distill-Qwen-32B Table 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks. RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1- Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger- scale reinforcement learning. 4.2. Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc- cess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not con- ducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil- ver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an'},\n",
       " {'metadata': {'page_number': 16, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'exponentially larger search space. To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly influences the quality of generation since it guides each step of the search process. Training a fine-grained value model is inherently difficult, which makes it challenging for the model to iteratively improve. While AlphaGo s core success relied on training a value model to progressively enhance its performance, this principle proves difficult to replicate in our setup due to the complexities of token generation. In conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a significant challenge. 5. Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction- tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1.   General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.   Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future   Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.   Software Engineering Tasks: Due to the long evaluation times, which impact the effi- ciency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency.'},\n",
       " {'metadata': {'page_number': 17, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'AI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m odels/blob/main/models/llama3_1/MODEL_CARD.md. Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3 M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. URL https: //arxiv.org/abs/2309.17179. L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760. A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or g/10.48550/arXiv.2406.04127. Google. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno logy/ai/google-gemini-next-generation-model-february-2024. Y. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi- nese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023. N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974.'},\n",
       " {'metadata': {'page_number': 18, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR, abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485 50/arXiv.2409.12941. A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur- ing massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let s verify step by step. arXiv preprint arXiv:2305.20050, 2023. B. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL https://github.com/WildEval/ZeroEval. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math -competitions/american-invitational-mathematics-examination-aime. OpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/. OpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin g-to-reason-with-llms/. OpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing -simpleqa/. OpenAI. Introducing SWE-bench verified we re releasing a human-validated subset of swe- bench that more, 2024d. URL https://openai.com/index/introducing-swe-bench -verified/. Qwen. Qwq: Reflect deeply on the boundaries of the unknown, 2024a. URL https://qwenlm .github.io/blog/qwq-32b-preview/. Qwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b log/qwen2.5. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017a. URL http://arxiv.org/abs/1712.01815.'},\n",
       " {'metadata': {'page_number': 19, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge. Nat., 550(7676):354 359, 2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270. C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033 T. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5. J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label- free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024. URL https://doi.org/10.48550/arXiv.2406.01574. C. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024. H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao, Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL https://arxiv.org/abs/2408.08152. J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.'},\n",
       " {'metadata': {'page_number': 20, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'A. Contributions and Acknowledgments Core Contributors Dejian Yang Haowei Zhang Junxiao Song Ruoyu Zhang Xiaokang Zhang Zhihong Shao Contributors Bingxuan Wang Chenggang Zhao Chengqi Deng Fangyun Lin Guangbo Hao Guanting Chen Honghui Ding Jianzhong Guo Jingchang Chen Jingyang Yuan Kaichao You Kexin Huang Lecong Zhang Litong Wang Liyue Zhang Mingchuan Zhang Minghua Zhang Minghui Tang Mingxu Zhou Miaojun Wang Mingming Li Panpan Huang Qiancheng Wang Ruisong Zhang'},\n",
       " {'metadata': {'page_number': 21, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'Shanghao Lu Shangyan Zhou Shanhuang Chen Shengfeng Ye Shuiping Yu Shunfeng Zhou Shuting Pan Shuang Zhou Shaoqing Wu Shengfeng Ye Wangding Zeng Wenfeng Liang Wentao Zhang Xiaodong Liu Xiaohan Wang Xiaokang Chen Xiaotao Nie Xingchao Liu Xuecheng Su Xiangyue Jin Xiaojin Shen Xiaosha Chen Xiaowen Sun Xiaoxiang Wang Xinnan Song Xianzu Wang Xinxia Shan Yaofeng Sun Yaohui Wang Yichao Zhang Yiliang Xiong Yisong Wang Yongqiang Guo Yuduan Wang Yunfan Xiong Yuxiang Luo Yuxiang You Yuyang Zhou Yanping Huang Zhangli Sha Zhengyan Zhang Zhicheng Ma Zhigang Yan'},\n",
       " {'metadata': {'page_number': 22, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'text': 'Ziyang Song Zizheng Pan Zhongyu Zhang Within each role, authors are listed alphabetically by the first name. Names marked with * denote individuals who have departed from our team.'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "641a8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "import hashlib\n",
    "\n",
    "def get_text_splitter(strategy=\"recursive\", chunk_size=800, chunk_overlap=150, model_name=\"gpt-4o\"):\n",
    "    if strategy == \"token\":\n",
    "        return TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, model_name=model_name)\n",
    "    # default: character recursive (works well)\n",
    "    return RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "splitter = get_text_splitter(strategy=\"recursive\", chunk_size=800, chunk_overlap=200)\n",
    "\n",
    "def generate_chunk_id(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a stable unique ID for a text chunk using file name, page, and text hash.\n",
    "    \"\"\"\n",
    "    # short hash of text to ensure uniqueness even if chunk sizes vary\n",
    "    text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "    return text_hash\n",
    "\n",
    "\n",
    "def chunking_doc(documents, splitter=splitter):\n",
    "    chunking_dataset = []\n",
    "    for document in documents:\n",
    "        texts = splitter.split_text(document[\"text\"])\n",
    "        for text in texts:\n",
    "            chunking_dataset.append({\n",
    "                \"text\": text,\n",
    "                \"metadata\": document[\"metadata\"],\n",
    "                \"id\": generate_chunk_id(\n",
    "                    text=text\n",
    "                )\n",
    "            })\n",
    "    return chunking_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "acdd2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_doc = chunking_doc(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "388a46fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 123)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs), len(chunk_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "44718fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'data science learning - LLM training and inference i want to learn ,how to large language model work , how to train or fine-tune and how to make efficient model inference , internel architecture and any other components Great   let s build a structured roadmap so you can deeply understand how large language models (LLMs) work, how they are trained/fine-tuned, and how inference is optimized. I ll break this into digestible parts, from foundations to advanced topics. 1. Foundations of LLMs Before diving into training and optimization, you need to be comfortable with the core building blocks. Neural Networks Basics Perceptron   Multi-Layer Perceptron (MLP) Backpropagation & Gradient Descent Sequence Models Evolution RNNs, LSTMs, GRUs   limitations (vanishing gradients, poor long- range',\n",
       "  'metadata': {'page_number': 1,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'cc3d884f6866ceb2b61c28e45ba9ebe8'},\n",
       " {'text': 'Neural Networks Basics Perceptron   Multi-Layer Perceptron (MLP) Backpropagation & Gradient Descent Sequence Models Evolution RNNs, LSTMs, GRUs   limitations (vanishing gradients, poor long- range memory) Self-Attention introduced in  Attention is All You Need  (2017) Transformers Encoder, Decoder, Encoder-Decoder Self-Attention mechanism: Query, Key, Value Positional Encodings Multi-Head Attention Feed-Forward layers, Residual Connections, Layer Norm',\n",
       "  'metadata': {'page_number': 1,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'b624f6a63ed04cef3a34eb1baba93517'},\n",
       " {'text': 'Action for you: Try to implement a tiny Transformer from scratch in PyTorch or TensorFlow. 2. Pretraining of LLMs LLMs like GPT, LLaMA, Falcon, Mistral are trained in two main stages. Pretraining Objective Usually causal language modeling (CLM)   predict next token. On massive datasets (hundreds of billions of tokens). Training Pipeline Tokenization (BPE, SentencePiece, WordPiece) Datasets (Common Crawl, books, code, Wikipedia, etc.) Loss Function: Cross-Entropy Optimization: AdamW, learning rate warm-up, cosine decay Mixed Precision Training (FP16/BF16) Gradient Accumulation, Gradient Clipping Distributed Training: Data Parallelism, Model Parallelism, Pipeline Parallelism   Deep dive topics: scaling laws, compute-data tradeoff, Chinchilla optimal 3. Fine-Tuning LLMs After pretraining,',\n",
       "  'metadata': {'page_number': 2,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'db42cdb1dcc7e1a11335626df2e5b8b7'},\n",
       " {'text': 'Distributed Training: Data Parallelism, Model Parallelism, Pipeline Parallelism   Deep dive topics: scaling laws, compute-data tradeoff, Chinchilla optimal 3. Fine-Tuning LLMs After pretraining, models are specialized. Full Fine-Tuning Update all model parameters (costly for big LLMs). Parameter-Efficient Fine-Tuning (PEFT) LoRA (Low-Rank Adaptation) Prefix Tuning, P-Tuning, Adapters',\n",
       "  'metadata': {'page_number': 2,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'bf2715db164dedc02eb990ad31683624'},\n",
       " {'text': 'QLoRA (quantized LoRA) for resource-constrained training Instruction Tuning Supervised Fine-Tuning (SFT) on curated instruction-response pairs Reinforcement Learning with Human Feedback (RLHF) Reward model from human preferences Proximal Policy Optimization (PPO) or DPO (Direct Preference Optimization)   Action for you: Try fine-tuning a small LLaMA/Mistral model using Hugging Face peft and trl libraries. 4. Inference Optimization (Efficient Serving) Running LLMs in production is another challenge. Quantization FP32   FP16   INT8   INT4 (trade-off between accuracy and speed/memory) Pruning & Distillation Remove redundant weights or distill into smaller models Batching & KV-Caching Reuse attention key-value states for faster autoregressive decoding Efficient batching across users Serving',\n",
       "  'metadata': {'page_number': 3,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'ae307d3fde66cba4ef6526ac589726cd'},\n",
       " {'text': 'Distillation Remove redundant weights or distill into smaller models Batching & KV-Caching Reuse attention key-value states for faster autoregressive decoding Efficient batching across users Serving Frameworks vLLM, TensorRT-LLM, DeepSpeed-Inference, Hugging Face text- generation-inference Streaming & Speculative Decoding Stream tokens while computing Use smaller draft models to speed up larger ones   Action for you: Experiment with quantized LLaMA 2/3 using bitsandbytes or auto-gptq and measure inference speed vs accuracy.',\n",
       "  'metadata': {'page_number': 3,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'd256b0fde39a8f8e574436903445f73d'},\n",
       " {'text': '5. Internal Architecture & Components When people say  how LLMs work internally,  they usually mean: Embedding Layer   converts tokens to vectors. Stack of Transformer Blocks   attention + feedforward layers. Layer Norm & Residuals   stabilize training. Output Layer (LM Head)   predicts token logits. Parameter Count vs Context Length   scaling considerations. 6. Advanced Topics Mixture of Experts (MoE)   only activate subset of weights per token. Retrieval-Augmented Generation (RAG)   use external vector DB for Memory & Context Window Extensions   FlashAttention, RoPE, ALiBi, sliding window attention. Evaluation Metrics   perplexity, BLEU, MMLU, HELM, MT-Bench.   Suggested Learning Path: 1. Learn Transformers basics   build toy model. 2. Explore pretraining pipeline   try pretraining on',\n",
       "  'metadata': {'page_number': 4,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': '257df10d30133319c872686a59c68b88'},\n",
       " {'text': 'attention. Evaluation Metrics   perplexity, BLEU, MMLU, HELM, MT-Bench.   Suggested Learning Path: 1. Learn Transformers basics   build toy model. 2. Explore pretraining pipeline   try pretraining on small dataset. 3. Learn fine-tuning (LoRA/QLoRA, SFT, RLHF)   practice with open-source 4. Dive into inference optimization   experiment with quantization & 5. Explore advanced architectures (MoE, RAG, long context).',\n",
       "  'metadata': {'page_number': 4,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': '5a04406a4df8313c9b1a3143313b3462'},\n",
       " {'text': 'Would you like me to make a step-by-step curriculum (with resources, papers, and small coding projects at each stage), so you can follow along like a structured learning plan? Perfect     let s build you a step-by-step curriculum for learning how large language models (LLMs) work, how to train/fine-tune them, and how to make inference efficient. I ll mix theory, papers, and coding projects so you get both intuition and hands-on skills.   Step-by-Step Curriculum for LLMs Stage 1: Core Foundations Goal: Build intuition about neural nets, attention, and Transformers. Concepts to Learn Neural nets basics (MLPs, backpropagation, optimization) Word embeddings & tokenization RNNs   LSTMs   Attention mechanism Transformer architecture Key Resources The Illustrated Transformer (visual explanation)',\n",
       "  'metadata': {'page_number': 5,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': '5165de84cf235d39d1ee551e4c644d4d'},\n",
       " {'text': 'basics (MLPs, backpropagation, optimization) Word embeddings & tokenization RNNs   LSTMs   Attention mechanism Transformer architecture Key Resources The Illustrated Transformer (visual explanation) Attention Is All You Need (2017)   original Transformer paper Stanford CS324: Large Language Models (lectures on YouTube) Coding Projects Implement a tiny Transformer (2 3 layers) in PyTorch.',\n",
       "  'metadata': {'page_number': 5,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'e7d4b06581aa94d1e845aec5f81aac48'},\n",
       " {'text': 'Train it on a toy dataset (e.g., character-level text generation on Shakespeare). Stage 2: Pretraining Pipeline Goal: Understand how GPT-like models are trained. Concepts to Learn Causal language modeling (CLM) objective Tokenization (BPE, SentencePiece) Training loops: batching, loss (cross-entropy), optimizers (AdamW) Scaling laws (Chinchilla, compute-data tradeoff) Distributed training (data parallelism, model parallelism) Key Resources Hugging Face Course: Transformers Scaling Laws for Neural Language Models (Kaplan et al., 2020) Training Compute-Optimal Large Language Models (DeepMind Chinchilla, 2022) Coding Projects Tokenize and pretrain a GPT-2-sized model on a small dataset (WikiText-2). Use Hugging Face transformers + datasets . Stage 3: Fine-Tuning & Adaptation Goal: Learn how',\n",
       "  'metadata': {'page_number': 6,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'e8758702527fb87de2075c0baf4e40bd'},\n",
       " {'text': '2022) Coding Projects Tokenize and pretrain a GPT-2-sized model on a small dataset (WikiText-2). Use Hugging Face transformers + datasets . Stage 3: Fine-Tuning & Adaptation Goal: Learn how to make pretrained models useful for tasks. Concepts to Learn Full fine-tuning vs parameter-efficient fine-tuning (PEFT) LoRA, QLoRA, adapters, prefix tuning Instruction tuning with curated datasets',\n",
       "  'metadata': {'page_number': 6,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'ddb3c8610a5539c8238d1bba087efc96'},\n",
       " {'text': 'RLHF (reinforcement learning with human feedback): PPO, DPO Key Resources LoRA: Low-Rank Adaptation of LLMs (Hu et al., 2021) Hugging Face PEFT library docs InstructGPT paper (OpenAI, 2022) Coding Projects Fine-tune a small model (LLaMA-7B or Mistral-7B) on a custom dataset with LoRA. Try QLoRA for GPU memory efficiency. Run supervised fine-tuning (SFT) on Alpaca dataset. Stage 4: Inference & Deployment Goal: Make LLMs run fast and cheap in production. Concepts to Learn Quantization (FP16   INT8   INT4) KV-caching for autoregressive decoding Efficient batching & streaming Speculative decoding (small model drafts for big models) Serving frameworks: vLLM, DeepSpeed-Inference, TGI Key Resources vLLM docs (vllm.ai Hugging Face Text Generation Inference Accelerating Inference with Speculative',\n",
       "  'metadata': {'page_number': 7,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'cf20f15bbb59bfb820b521a6b9b239ba'},\n",
       " {'text': '(small model drafts for big models) Serving frameworks: vLLM, DeepSpeed-Inference, TGI Key Resources vLLM docs (vllm.ai Hugging Face Text Generation Inference Accelerating Inference with Speculative Decoding (Leviathan et al., 2023) Coding Projects Quantize a model with bitsandbytes or auto-gptq . Serve a model with vLLM and benchmark throughput. Implement streaming token generation.',\n",
       "  'metadata': {'page_number': 7,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': '1779ed61188aa0337cdaa31a306350d9'},\n",
       " {'text': 'Stage 5: Advanced Architectures & Research Goal: Explore cutting-edge improvements. Concepts to Learn Mixture of Experts (MoE) models (Switch Transformer, Mixtral) Long-context attention (RoPE, ALiBi, FlashAttention) Retrieval-Augmented Generation (RAG) Evaluation frameworks: MMLU, HELM, MT-Bench Key Resources Switch Transformers (Fedus et al., 2021) FlashAttention (Dao et al., 2022) Retrieval-Augmented Generation (Lewis et al., 2020) Stanford HELM benchmark Coding Projects Add retrieval (RAG) to an open-source LLM using FAISS/Chroma DB. Experiment with long-context models (Mistral-Long, LLaMA-Long). Fine-tune a reward model for preferences (mini-RLHF experiment). Stage 6: Putting It All Together Goal: Build an end-to-end mini-LLM project. Choose a domain (e.g., finance, medical Q&A,',\n",
       "  'metadata': {'page_number': 8,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'b6e4a9752673161b7bb578a0fffe9d84'},\n",
       " {'text': 'LLaMA-Long). Fine-tune a reward model for preferences (mini-RLHF experiment). Stage 6: Putting It All Together Goal: Build an end-to-end mini-LLM project. Choose a domain (e.g., finance, medical Q&A, education). Pretrain a small model on a domain dataset. Fine-tune with LoRA for instruction following. Optimize inference with quantization + caching. Add RAG for domain-specific knowledge. Deploy via API.',\n",
       "  'metadata': {'page_number': 8,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': '9d4d04fa0175aa2f6979925e78206508'},\n",
       " {'text': 'By the end, you ll understand internal architecture, training, fine-tuning, inference efficiency, and advanced methods. Do you want me to turn this curriculum into a 12-week learning plan (weekly breakdown with reading + coding tasks), so you can follow it like a structured course? explain basic term from NN, LLM and quantazation , explain Absolutely   let s start by explaining the basic terms you ll encounter in Neural Networks (NNs), Large Language Models (LLMs), and Quantization   in a clear, intuitive way with short examples.   1. Basic Terms from Neural Networks (NN) These are the building blocks behind all deep learning, including LLMs. A neuron takes inputs, multiplies them by weights, adds a bias, and applies an activation function. Think of it as a single  decision unit . output',\n",
       "  'metadata': {'page_number': 9,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': '4b127e8de4301d8b8f49d94f15544897'},\n",
       " {'text': 'blocks behind all deep learning, including LLMs. A neuron takes inputs, multiplies them by weights, adds a bias, and applies an activation function. Think of it as a single  decision unit . output = activation(w1*x1 + w2*x2 + b)',\n",
       "  'metadata': {'page_number': 9,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': '9956d170d88d12141ea02d79a13b4a0d'},\n",
       " {'text': 'A collection of neurons stacked together. Input layer: Takes raw data (like words, images, etc.) Hidden layers: Learn patterns Output layer: Produces final prediction 1.3 Weights and Biases Weights (w): Control the strength of connections between neurons. Bias (b): Allows shifting the activation threshold. These are the parameters the network learns during training. 1.4 Activation Function Adds non-linearity, so the network can learn complex patterns. Common types: ReLU (Rectified Linear Unit): max(0, x) Sigmoid: 1 / (1 + e^-x) Tanh: outputs between -1 and 1 1.5 Loss Function Measures how far the model s predictions are from the correct answers. Example for classification: Cross-Entropy Loss 1.6 Backpropagation Algorithm to adjust weights using gradients.',\n",
       "  'metadata': {'page_number': 10,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': '26468c0b068cbb8cc9aadedc7eec16ea'},\n",
       " {'text': 'It calculates how changing each weight will affect the loss, then updates the weights to minimize the error. 1.7 Optimizer Controls how weights are updated. Common optimizers: SGD (Stochastic Gradient Descent) Adam / AdamW (used in LLMs)   2. Basic Terms from Large Language Models LLMs are specialized neural networks (Transformers) trained on text to predict the next word or token. A piece of text (can be a word, subword, or character).  Playing    [\"Play\", \"ing\"] Tokenization converts text into numerical IDs so the model can process 2.2 Embedding A vector representation of a token.',\n",
       "  'metadata': {'page_number': 11,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'ecc04ed7357713cfae0d0bdbd3e6eb64'},\n",
       " {'text': 'The word  king  might be represented as [0.23, -0.41, 0.87, ...] Similar words have nearby embeddings. 2.3 Transformer The core architecture behind LLMs. Built from self-attention layers and feed-forward networks. 2.4 Attention Mechanism Allows the model to  focus  on relevant parts of the input sequence. In the sentence  The cat sat on the mat,  when predicting  mat,  the model attends to  sat  and  on . 2.5 Self-Attention (Q, K, V) Each token gets: Query (Q): what it s looking for Key (K): what it offers Value (V): information it provides The model computes attention weights between Q and K to decide how much each token should influence the others. 2.6 Positional Encoding Since Transformers don t process data sequentially, positional encodings tell the model the order of tokens in the',\n",
       "  'metadata': {'page_number': 12,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': '7f16b19596fe1631f2def62b93072baf'},\n",
       " {'text': 'K to decide how much each token should influence the others. 2.6 Positional Encoding Since Transformers don t process data sequentially, positional encodings tell the model the order of tokens in the sentence.',\n",
       "  'metadata': {'page_number': 12,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'ac27235d95fb31e9a073920ab7f74663'},\n",
       " {'text': '2.7 Feed-Forward Network (FFN) A small neural network inside each Transformer block that processes token representations after attention. 2.8 Layer Normalization & Residuals LayerNorm: stabilizes training by normalizing activations. Residual connections: help preserve information by adding inputs back to outputs ( x + f(x) ). 2.9 LM Head (Language Modeling Head) Final layer that converts hidden vectors into probabilities over vocabulary (predicting the next token). 2.10 Context Window The number of tokens the model can  see  at once. GPT-3: 2048 tokens GPT-4: up to 128K or more   3. Basic Terms from Quantization Quantization is about making models smaller and faster for inference. 3.1 Precision Refers to how many bits are used to store a number. FP32 = 32-bit floating point',\n",
       "  'metadata': {'page_number': 13,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'eb396ecc278ee9696fbcab893ea734d6'},\n",
       " {'text': 'FP16 = 16-bit INT8 / INT4 = 8-bit or 4-bit integer Smaller precision   less memory, faster computation (but may reduce 3.2 Quantization The process of converting model weights (and sometimes activations) from high precision (FP32) to lower precision (INT8, INT4). 3.3 Types of Quantization Post-Training Quantization (PTQ): Quantize after training. Quantization-Aware Training (QAT): Simulate quantization during training for better accuracy. 3.4 Dynamic vs Static Quantization Dynamic: Quantize weights only during inference. Static: Quantize both weights and activations ahead of time. FP32 weight: 0.324582934 Quantized INT8: 0.32',\n",
       "  'metadata': {'page_number': 14,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': '9cfc0bdd1a71eac3dc8ee9d2e40f67cc'},\n",
       " {'text': 'Combines quantization and LoRA (Low-Rank Adaptation). Allows fine-tuning large models (like LLaMA 70B) efficiently on small 3.6 Trade-Off   Small drop   Very fast   Very small   More loss 3.7 Quantized Inference Using quantized weights in production to serve models efficiently. Libraries: bitsandbytes , auto-gptq , ggml , exllama , vLLM .   In short: Neural Networks = how learning happens LLMs = what structure learns text Quantization = how to make models faster and smaller Would you like me to now draw a visual map (like a diagram explaining how all these components fit together inside an LLM   from input tokens',\n",
       "  'metadata': {'page_number': 15,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'b61b1814990593e4dfdd27f60e49b165'},\n",
       " {'text': 'embeddings   transformer layers   quantized output)? explain lora in simple word Absolutely     let s explain LoRA (Low-Rank Adaptation) in very simple terms, step by step.   What Is LoRA? LoRA stands for Low-Rank Adaptation   it s a method to fine-tune large language models (LLMs) without changing most of their weights. It was introduced to make fine-tuning big models cheap and efficient  .   Imagine This Scenario You have a huge model like LLaMA 7B (7 billion parameters). Fine-tuning all its parameters would: Need lots of GPU memory Take days or weeks Risk forgetting what it already knows (catastrophic forgetting) So instead of updating all weights, LoRA says:    Let s keep the original model frozen and just learn some small extra   How It Works (Simplified)',\n",
       "  'metadata': {'page_number': 16,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'b8e62cad356a9f9a3d8b465cb361fa5e'},\n",
       " {'text': 'Every Transformer layer has big matrices   for example, W   that help it process information. Normally, fine-tuning means updating this entire matrix (maybe millions of So LoRA replaces: A and B are small matrices (much smaller than W). Only A and B are trained. W_original is frozen (unchanged).   Why  Low-Rank ? The term low-rank just means that A   B is a small adjustment   it has much fewer parameters than W. If W is size 1000 1000 (1,000,000 parameters), LoRA might make A (1000 4) and B (4 1000), which is only 8,000 parameters   125  smaller!   So What Happens in Practice  We don t need to change all these numbers   we can just learn a small correction.  W_new = W_original + (A   B)',\n",
       "  'metadata': {'page_number': 17,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': '2e1913f87c67a0fa13a13644eb3d24d2'},\n",
       " {'text': '1. You load a pretrained model (e.g., LLaMA, Mistral). 2. Freeze all weights so they don t change. 3. Insert LoRA adapters (the small A and B matrices). 4. Train only the adapters on your new dataset (like chat, finance, etc.). 5. When generating text, the model uses both the base weights and LoRA s learned updates.   Benefits of LoRA   Few trainable Only train a small part   saves You can load multiple LoRAs for different tasks Fine-tune huge models on a   Mergeable LoRA weights can be merged back into the base model later   Example in Hugging Face (Python) from peft import LoraConfig, get_peft_model from transformers import AutoModelForCausalLM # Load base model model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")',\n",
       "  'metadata': {'page_number': 18,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': '9a6593a92bcd1c9dc83acf17a363316c'},\n",
       " {'text': 'This modifies the model without touching the base weights. Think of LoRA like putting sticky notes on top of a textbook. The textbook = pretrained model (frozen) The sticky notes = LoRA adapters (new info) You re adding knowledge without rewriting the whole book!   In short: Would you like me to explain QLoRA next (it s the  quantized  version of LoRA that makes it even faster and lighter)? # Define LoRA settings config = LoraConfig( r=8, # rank (how small the adapter is) lora_alpha=32, # scaling factor target_modules=[\"q_proj\", \"v_proj\"], # layers to inject LoRA into lora_dropout=0.05 # Add LoRA adapters model = get_peft_model(model, config) LoRA lets you fine-tune giant models by training just a few extra, small matrices   it s cheap, efficient, and modular.',\n",
       "  'metadata': {'page_number': 19,\n",
       "   'source': '../data/data science learning - LLM training and inference.pdf'},\n",
       "  'id': 'cfe5c0856488873f65925deca8c75bde'},\n",
       " {'text': 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning DeepSeek-AI research@deepseek.com We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super- vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek- R1 achieves performance comparable to',\n",
       "  'metadata': {'page_number': 1, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '5ed9c492191c6a5986ec8cfe9af2ff9b'},\n",
       " {'text': 'issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek- R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama. (Percentile) GPQA Diamond SWE-bench Verified Accuracy / Percentile (%) DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3 Figure 1 | Benchmark performance of DeepSeek-R1. arXiv:2501.12948v1 [cs.CL] 22 Jan 2025',\n",
       "  'metadata': {'page_number': 1, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'b2b486bf214aaf2a84d1b2069d280a96'},\n",
       " {'text': 'Introduction Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . . Reinforcement Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Training Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . . Cold Start . . . . . . . . . . . . . . . .',\n",
       "  'metadata': {'page_number': 2, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '21af96dc89d8ee66129759b8e11549dd'},\n",
       " {'text': '. . Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . . Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . . Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . . Reinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . . . . Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . . DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Distilled Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Distillation v.s. Reinforcement Learning . . . . . . . . . . . . .',\n",
       "  'metadata': {'page_number': 2, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '0e7250075b8b8e7d1dbc7e5b97bd0d9b'},\n",
       " {'text': '. . . . . . . . . . . . . . . . . . . . . . Distilled Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Conclusion, Limitations, and Future Work A Contributions and Acknowledgments',\n",
       "  'metadata': {'page_number': 2, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'a7b74b710b68f2cd71713b16c86af99a'},\n",
       " {'text': '1. Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI s o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of- Thought reasoning process. This approach has achieved significant improvements in',\n",
       "  'metadata': {'page_number': 3, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '43bf8b01e583a9e0cbe70079cb6a0862'},\n",
       " {'text': '2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of- Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable',\n",
       "  'metadata': {'page_number': 3, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '58cf3bdcd049d458e8aee54462055e40'},\n",
       " {'text': 'such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI s o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After',\n",
       "  'metadata': {'page_number': 3, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '1b16e08487b58ac1ba8af7137261c5e7'},\n",
       " {'text': 'et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of',\n",
       "  'metadata': {'page_number': 3, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'eb1036b70a82f62482adaaf8ad4ddae7'},\n",
       " {'text': 'enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1- Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as',\n",
       "  'metadata': {'page_number': 3, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '417af0148949aec4207cdfc2923c58df'},\n",
       " {'text': 'After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5- 32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru- cial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the',\n",
       "  'metadata': {'page_number': 3, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'e2c03612d283dabe4cf1456ae9119445'},\n",
       " {'text': 'the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models.',\n",
       "  'metadata': {'page_number': 3, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'bb249980dfe83c96cd7ffd2aa44fc414'},\n",
       " {'text': '1.1. Contributions Post-Training: Large-Scale Reinforcement Learning on the Base Model   We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek- R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.   We introduce our pipeline to develop DeepSeek-R1. The pipeline',\n",
       "  'metadata': {'page_number': 4, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'd758978ea8e98534727f869883ab32ff'},\n",
       " {'text': 'can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human pref- erences, as well as two SFT stages that serve as the seed for the model s reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. Distillation: Smaller Models Can Be Powerful Too   We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its',\n",
       "  'metadata': {'page_number': 4, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'fbf7334949f7d3d38b964f73aacf6aa1'},\n",
       " {'text': 'models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.   Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek- R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi- tionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-',\n",
       "  'metadata': {'page_number': 4, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '1e794cf54885ba299ac99a098fa42e2d'},\n",
       " {'text': 'QwQ-32B-Preview. Addi- tionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open- source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. 1.2. Summary of Evaluation Results   Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-o1-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces',\n",
       "  'metadata': {'page_number': 4, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '7f42d012f681b6e28a4aceeb4d017fd2'},\n",
       " {'text': 'and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks.   Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek- R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks.',\n",
       "  'metadata': {'page_number': 4, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '3e196bbb81c048fdcae0706f848b6188'},\n",
       " {'text': 'Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.',\n",
       "  'metadata': {'page_number': 4, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '3fe0213f2c26926fcd51974a71c46f2d'},\n",
       " {'text': 'Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are- naHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. 2. Approach 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even',\n",
       "  'metadata': {'page_number': 5, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'd051cbe0740e33ae15fc953d86653e18'},\n",
       " {'text': 'amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. 2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has',\n",
       "  'metadata': {'page_number': 5, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '6fd41b1b4927d9a90abdd8e2b1249843'},\n",
       " {'text': 'Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. 2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev- idenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. 2.2.1.',\n",
       "  'metadata': {'page_number': 5, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'b0195796ea11f5694cbb9d6f4870b342'},\n",
       " {'text': 'learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. 2.2.1. Reinforcement Learning Algorithm Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question q, GRPO samples a group of outputs {o1, o2,       , oG} from the old policy  oldand then optimizes the policy model  by maximizing the following objective: JGRPO( ) = E[q P(Q), {oi}G i=1  old(O|q)]  old(oi|q) Ai, clip  old(oi|q) , 1  , 1 +    (oi|q)',\n",
       "  'metadata': {'page_number': 5, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'aab3f0cba51f824a0cbee79e4c44c750'},\n",
       " {'text': 'oG} from the old policy  oldand then optimizes the policy model  by maximizing the following objective: JGRPO( ) = E[q P(Q), {oi}G i=1  old(O|q)]  old(oi|q) Ai, clip  old(oi|q) , 1  , 1 +    (oi|q)  log  (oi|q)  1, where  and  are hyper-parameters, and Aiis the advantage, computed using a group of rewards {r1, r2, . . . , rG} corresponding to the outputs within each group: Ai= ri mean({r1, r2,       , rG}) std({r1, r2,       , rG})',\n",
       "  'metadata': {'page_number': 5, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'aec0148431a284360c558d641d5c2422'},\n",
       " {'text': 'A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within and tags, respectively, i.e., reasoning process here answer here . User: prompt. Assistant: Table 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning question during training. 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:   Accuracy rewards: The accuracy reward model evaluates whether the response is correct. For',\n",
       "  'metadata': {'page_number': 6, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'c8ad598af7dfe8260cc92f5f97c91c8d'},\n",
       " {'text': 'train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:   Accuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.   Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between   and   We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that',\n",
       "  'metadata': {'page_number': 6, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '48942d34d4be2cf7bef626e6a153ea91'},\n",
       " {'text': 'format reward model that enforces the model to put its thinking process between   and   We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding',\n",
       "  'metadata': {'page_number': 6, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'f745f674bbd662c4238d6a9a511a1b06'},\n",
       " {'text': 'in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biases such as mandating reflective reasoning or promoting particular problem-solving strate- gies to ensure that we can accurately observe the model s natural progression during the RL 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek- R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score',\n",
       "  'metadata': {'page_number': 6, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '0ebc69d8865387a578232cde1d945607'},\n",
       " {'text': 'throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the model s performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI s o1-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers',\n",
       "  'metadata': {'page_number': 6, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'dc8423101328ac0514bd960d2933fd15'},\n",
       " {'text': 'OpenAI-o1-mini OpenAI-o1-0912 DeepSeek-R1-Zero Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the model s ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek- R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero s performance escalates from 71.0% to',\n",
       "  'metadata': {'page_number': 7, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '28b515a41927e284f792f88571cbe1ed'},\n",
       " {'text': 'can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero s performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the model s progression without the influence of the',\n",
       "  'metadata': {'page_number': 7, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'fb52a209ab7fd95a3ccbb11e8710518d'},\n",
       " {'text': 'how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the model s progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-',\n",
       "  'metadata': {'page_number': 7, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'e16db231650750131e59589b84113650'},\n",
       " {'text': 'Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. ment throughout the training process. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time compu- tation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such',\n",
       "  'metadata': {'page_number': 8, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'aece24bafb72a193e77a92f466e2eba0'},\n",
       " {'text': 'its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflection where the model revisits and reevaluates its previous steps and the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the model s interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zero s reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of',\n",
       "  'metadata': {'page_number': 8, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'ab270e82653cb8c5bfb4398d9584f78e'},\n",
       " {'text': 'capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an  aha moment . This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the model s growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an  aha moment  for the model but also for the researchers observing its behavior. It underscores the power and',\n",
       "  'metadata': {'page_number': 8, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '24804030cfab230184a8b3da8d02a613'},\n",
       " {'text': 'learning can lead to unexpected and sophisticated outcomes. This moment is not only an  aha moment  for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The  aha moment  serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.',\n",
       "  'metadata': {'page_number': 8, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'dd630e4a65fe610da229e007d5b72c47'},\n",
       " {'text': 'Question: If a> 1, then the sum of the real solutions of a+ x= xis equal to Response: To solve the equation a+ x= x, let s start by squaring both       Rearrange to isolate the inner square root term: (a x2)2 = a+ x= a2  2ax2 + (x2)2 = a+ x= x4  2ax2  x+ (a2  a) = 0 Wait, wait. Wait. That s an aha moment I can flag here. Let s reevaluate this step-by-step to identify if the correct sum can be       We started with the equation: First, let s square both sides: a+ x= x2 =  Next, I could square both sides again, treating the equation:       Table 3 | An interesting  aha moment  of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning.',\n",
       "  'metadata': {'page_number': 9, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '7411c29803cd198c75159771e9704abf'},\n",
       " {'text': 'version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. 2.3. DeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1)',\n",
       "  'metadata': {'page_number': 9, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'd1ca76bea4802ceadd7c71fca3008207'},\n",
       " {'text': 'that utilizes RL with human-friendly cold-start data. 2.3. DeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. 2.3.1. Cold Start Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a',\n",
       "  'metadata': {'page_number': 9, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'ffa60e59e4a2ae82dc9d38e1dd1b8908'},\n",
       " {'text': 'stages, outlined as follows. 2.3.1. Cold Start Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1- Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data',\n",
       "  'metadata': {'page_number': 9, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '463411a9e4baff4adf87d243d8d9ac32'},\n",
       " {'text': 'Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as |special_token||special_token|, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning   Potential: By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. 2.3.2.',\n",
       "  'metadata': {'page_number': 10, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'eefc22c09aeed4db826fa3e7feffb643'},\n",
       " {'text': 'designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the model s reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language',\n",
       "  'metadata': {'page_number': 10, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '0c5e847e08f0d72ea3f1c8012629169b'},\n",
       " {'text': 'with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the model s performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. 2.3.3. Rejection Sampling and',\n",
       "  'metadata': {'page_number': 10, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'fece65f34ab5c6e37bc55d374db633e6'},\n",
       " {'text': 'consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model s capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. Reasoning data We curate reasoning prompts and generate reasoning trajectories by perform- ing rejection sampling from the checkpoint from the',\n",
       "  'metadata': {'page_number': 10, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'acc347c45633c06b2fa7f28748e2d31e'},\n",
       " {'text': 'the data and fine-tune the model as described below. Reasoning data We curate reasoning prompts and generate reasoning trajectories by perform- ing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about',\n",
       "  'metadata': {'page_number': 10, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '4bccade46e0b60fc25af30929e3b8c0b'},\n",
       " {'text': 'have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples.',\n",
       "  'metadata': {'page_number': 10, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '2d668b02a62a1b6aac8eb25c93fda1ff'},\n",
       " {'text': 'Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as  hello  we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model',\n",
       "  'metadata': {'page_number': 11, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'f3ebffa2badf32ca9bb466d6040169d2'},\n",
       " {'text': '800k samples. 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model s helpfulness and harmlessness while simultane- ously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train- ing',\n",
       "  'metadata': {'page_number': 11, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '19c619afafef40005cefddec5281fbcd'},\n",
       " {'text': 'we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train- ing prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning',\n",
       "  'metadata': {'page_number': 11, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '958264da41aef47dad53ce73a4df68bb'},\n",
       " {'text': 'or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in  2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5- 14B, Qwen2.5-32B,',\n",
       "  'metadata': {'page_number': 11, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '8ad4d81f216d37897591e77dc2ce17ce'},\n",
       " {'text': 'straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5- 14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. 3. Experiment We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al.,',\n",
       "  'metadata': {'page_number': 11, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '49bc448dfb0fff46468f227562091cdb'},\n",
       " {'text': 'the RL stage to the broader research community. 3. Experiment We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,',\n",
       "  'metadata': {'page_number': 11, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'b9692499427b4a7b60aa5a52e887d7a2'},\n",
       " {'text': '2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08   2025-01), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math- ematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. Evaluation Prompts Following the setup in',\n",
       "  'metadata': {'page_number': 12, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'bc6e85c2a0ecdd1777da0f7a6d875657'},\n",
       " {'text': 'to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple- evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight',\n",
       "  'metadata': {'page_number': 12, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '3d0151f35f1a3bd2d5b8f78863e111c7'},\n",
       " {'text': 'of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum',\n",
       "  'metadata': {'page_number': 12, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '8582b742d5b1e5a518988a5bd4e7466d'},\n",
       " {'text': 'SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor- mance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher',\n",
       "  'metadata': {'page_number': 12, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'c5e8cb493fe8f7bacfc4f4f122abccf3'},\n",
       " {'text': '(Qwen, 2024a). Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@kevaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-pvalue of 0.95 to generate k responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as where pidenotes the correctness of the i-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64',\n",
       "  'metadata': {'page_number': 12, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '1cf096b9670bf9e63923d7c92a6170b3'},\n",
       " {'text': 'pidenotes the correctness of the i-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. 1https://aider.chat 2https://codeforces.com 3https://www.cms.org.cn/Home/comp/comp/cid/12.html',\n",
       "  'metadata': {'page_number': 12, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'ea3ec87a2d781fe6682953db7fafe24e'},\n",
       " {'text': '3.1. DeepSeek-R1 Evaluation Benchmark (Metric) Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek Sonnet-1022 o1-mini o1-1217 Architecture # Activated Params # Total Params MMLU (Pass@1) MMLU-Redux (EM) MMLU-Pro (EM) DROP (3-shot F1) IF-Eval (Prompt Strict) GPQA Diamond (Pass@1) SimpleQA (Correct) FRAMES (Acc.) AlpacaEval2.0 (LC-winrate) ArenaHard (GPT-4-1106) LiveCodeBench (Pass@1-COT) Codeforces (Percentile) Codeforces (Rating) SWE Verified (Resolved) Aider-Polyglot (Acc.) AIME 2024 (Pass@1) MATH-500 (Pass@1) CNMO 2024 (Pass@1) CLUEWSC (EM) C-Eval (EM) C-SimpleQA (Correct) Table 4 | Comparison between DeepSeek-R1 and other representative models. For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to',\n",
       "  'metadata': {'page_number': 13, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '499a9feb5376fb8257917bc1b87fffb1'},\n",
       " {'text': 'between DeepSeek-R1 and other representative models. For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im- provement is primarily attributed to enhanced accuracy in STEM-related questions, where signif- icant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses',\n",
       "  'metadata': {'page_number': 13, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'e975abcc47179254c4093eb7c32537f4'},\n",
       " {'text': 'tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a model s ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed',\n",
       "  'metadata': {'page_number': 13, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'a7e917144a97a60a3d3e485f5bcf1000'},\n",
       " {'text': 'improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1 s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that',\n",
       "  'metadata': {'page_number': 13, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'c10b68ca3ca3f52b11c285ce3e8fcb2d'},\n",
       " {'text': 'DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. 3.2. Distilled Model Evaluation GPT-4o-0513 Claude-3.5-Sonnet-1022 OpenAI-o1-mini',\n",
       "  'metadata': {'page_number': 14, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '00d100031a0185b2792e787f70c87039'},\n",
       " {'text': 'will improve in the next version, as the amount of related RL training data currently remains very limited. 3.2. Distilled Model Evaluation GPT-4o-0513 Claude-3.5-Sonnet-1022 OpenAI-o1-mini QwQ-32B-Preview DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-14B DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Llama-70B Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. As shown in Table 5, simply distilling DeepSeek-R1 s outputs enables the efficient DeepSeek- R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non- reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B- Preview on all evaluation metrics,',\n",
       "  'metadata': {'page_number': 14, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'be6d2690ec940bf835f7cceb9bf6f82f'},\n",
       " {'text': 'DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non- reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B- Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla- tion. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. 4. Discussion 4.1. Distillation v.s. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the',\n",
       "  'metadata': {'page_number': 14, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '23dfd9ab88b2e638fa019a3f49391419'},\n",
       " {'text': 'Distillation v.s. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale',\n",
       "  'metadata': {'page_number': 14, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '3a05b056206f688782becfc9e50c3099'},\n",
       " {'text': 'GPQA Diamond LiveCodeBench QwQ-32B-Preview DeepSeek-R1-Zero-Qwen-32B DeepSeek-R1-Distill-Qwen-32B Table 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks. RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1- Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the',\n",
       "  'metadata': {'page_number': 15, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'bfb570ceb23c5afe38f8f39964f23364'},\n",
       " {'text': 'this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger- scale reinforcement learning. 4.2. Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023).',\n",
       "  'metadata': {'page_number': 15, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'd9a8e3d36c61589a6103a9a912c00125'},\n",
       " {'text': 'models. Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc- cess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not con- ducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole',\n",
       "  'metadata': {'page_number': 15, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '8e6600979e9b73999a2c5ecb527defbb'},\n",
       " {'text': 'Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil- ver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller',\n",
       "  'metadata': {'page_number': 15, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '9ac1a8b46f0d54346d5cc81166e4b8c4'},\n",
       " {'text': 'et al., 2017b) and AlphaZero (Sil- ver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively',\n",
       "  'metadata': {'page_number': 15, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '30b5d743b2369c9e5e605df2e1a31bf3'},\n",
       " {'text': 'and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an',\n",
       "  'metadata': {'page_number': 15, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '21fec00373abb8b472eb36160c16335d'},\n",
       " {'text': 'exponentially larger search space. To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly influences the quality of generation since it guides each step of the search process. Training a fine-grained value model is inherently difficult, which makes it challenging for the model to iteratively improve. While AlphaGo s core success relied on training a value model to progressively enhance its performance, this principle proves difficult to replicate in our setup due to the complexities of token generation. In conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a',\n",
       "  'metadata': {'page_number': 16, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '094876ba9f183e217e77ab4739e2015d'},\n",
       " {'text': 'token generation. In conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a significant challenge. 5. Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the',\n",
       "  'metadata': {'page_number': 16, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'e239a83f4e1dc4a4d0726be7f22c254a'},\n",
       " {'text': 'DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction- tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1.   General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function',\n",
       "  'metadata': {'page_number': 16, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'c79fc927771583bc1c106dc3591c171d'},\n",
       " {'text': 'we plan to invest in research across the following directions for DeepSeek-R1.   General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.   Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future   Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts.',\n",
       "  'metadata': {'page_number': 16, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'bd0bfa02e946fbd7db1ad53eae883ffc'},\n",
       " {'text': 'the query is in a language other than English or Chinese. We aim to address this limitation in future   Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.   Software Engineering Tasks: Due to the long evaluation times, which impact the effi- ciency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering',\n",
       "  'metadata': {'page_number': 16, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'a165bdc25b9822e784e4e6b8e7b958a7'},\n",
       " {'text': 'DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency.',\n",
       "  'metadata': {'page_number': 16, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '89a371c87bee2f2accbe6a0dace7832b'},\n",
       " {'text': 'AI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m odels/blob/main/models/llama3_1/MODEL_CARD.md. Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3 M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P.',\n",
       "  'metadata': {'page_number': 17, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '4633d2ef2040b6f5f6e9d84f830d3514'},\n",
       " {'text': 'Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W.',\n",
       "  'metadata': {'page_number': 17, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '661db6f98cb64de31d6af72d257a404d'},\n",
       " {'text': 'P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. URL https: //arxiv.org/abs/2309.17179. L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760. A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or g/10.48550/arXiv.2406.04127. Google. Our next-generation',\n",
       "  'metadata': {'page_number': 17, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '123c2c5d39c06e6343ddf600516988b9'},\n",
       " {'text': 'R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or g/10.48550/arXiv.2406.04127. Google. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno logy/ai/google-gemini-next-generation-model-february-2024. Y. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi- nese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A',\n",
       "  'metadata': {'page_number': 17, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '1bdee294f407cd2c1e5bc95b27a50b31'},\n",
       " {'text': 'Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023. N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974.',\n",
       "  'metadata': {'page_number': 17, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'b6293d7c5842e57fdc72a802ecc4f284'},\n",
       " {'text': 'S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR, abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485 50/arXiv.2409.12941. A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur- ing massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data',\n",
       "  'metadata': {'page_number': 18, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'ffdcae8c771ed623ac04dbe9c125d041'},\n",
       " {'text': 'massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let s verify step by step. arXiv preprint arXiv:2305.20050, 2023. B. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL https://github.com/WildEval/ZeroEval. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math',\n",
       "  'metadata': {'page_number': 18, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '3b00c637142fa8de3e51747271b81fc4'},\n",
       " {'text': 'URL https://github.com/WildEval/ZeroEval. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math -competitions/american-invitational-mathematics-examination-aime. OpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/. OpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin g-to-reason-with-llms/. OpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing -simpleqa/. OpenAI. Introducing SWE-bench verified we re releasing a human-validated subset of swe- bench that more, 2024d. URL https://openai.com/index/introducing-swe-bench -verified/. Qwen. Qwq: Reflect deeply on the boundaries of the unknown, 2024a. URL https://qwenlm',\n",
       "  'metadata': {'page_number': 18, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '64f55c1e8ab0ac2d7bab2d307ec328c6'},\n",
       " {'text': 'subset of swe- bench that more, 2024d. URL https://openai.com/index/introducing-swe-bench -verified/. Qwen. Qwq: Reflect deeply on the boundaries of the unknown, 2024a. URL https://qwenlm .github.io/blog/qwq-32b-preview/. Qwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b log/qwen2.5. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L.',\n",
       "  'metadata': {'page_number': 18, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'af8d964f8642bcd6208bfdf0cc35dbcd'},\n",
       " {'text': 'Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017a. URL http://arxiv.org/abs/1712.01815.',\n",
       "  'metadata': {'page_number': 18, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '12823ff023f4746c90ba754e392a8818'},\n",
       " {'text': 'D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge. Nat., 550(7676):354 359, 2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270. C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033 T. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5. J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word',\n",
       "  'metadata': {'page_number': 19, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '46c5e2330a6965797d116749ac6214b9'},\n",
       " {'text': 'without human demonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5. J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label- free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A.',\n",
       "  'metadata': {'page_number': 19, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '56d2adccdfbc73d52802ae71f2696e9d'},\n",
       " {'text': 'of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024. URL https://doi.org/10.48550/arXiv.2406.01574. C. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024. H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao, Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL',\n",
       "  'metadata': {'page_number': 19, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '332384913f7f76d491e233203089dca1'},\n",
       " {'text': 'Lu, Q. Du, W. Gao, Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL https://arxiv.org/abs/2408.08152. J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.',\n",
       "  'metadata': {'page_number': 19, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '0ca97b5f97da49860f067ec6832a9c3f'},\n",
       " {'text': 'A. Contributions and Acknowledgments Core Contributors Dejian Yang Haowei Zhang Junxiao Song Ruoyu Zhang Xiaokang Zhang Zhihong Shao Contributors Bingxuan Wang Chenggang Zhao Chengqi Deng Fangyun Lin Guangbo Hao Guanting Chen Honghui Ding Jianzhong Guo Jingchang Chen Jingyang Yuan Kaichao You Kexin Huang Lecong Zhang Litong Wang Liyue Zhang Mingchuan Zhang Minghua Zhang Minghui Tang Mingxu Zhou Miaojun Wang Mingming Li Panpan Huang Qiancheng Wang Ruisong Zhang',\n",
       "  'metadata': {'page_number': 20, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '5d5365a2c0702bcd2fd74f9528c5f335'},\n",
       " {'text': 'Shanghao Lu Shangyan Zhou Shanhuang Chen Shengfeng Ye Shuiping Yu Shunfeng Zhou Shuting Pan Shuang Zhou Shaoqing Wu Shengfeng Ye Wangding Zeng Wenfeng Liang Wentao Zhang Xiaodong Liu Xiaohan Wang Xiaokang Chen Xiaotao Nie Xingchao Liu Xuecheng Su Xiangyue Jin Xiaojin Shen Xiaosha Chen Xiaowen Sun Xiaoxiang Wang Xinnan Song Xianzu Wang Xinxia Shan Yaofeng Sun Yaohui Wang Yichao Zhang Yiliang Xiong Yisong Wang Yongqiang Guo Yuduan Wang Yunfan Xiong Yuxiang Luo Yuxiang You Yuyang Zhou Yanping Huang Zhangli Sha Zhengyan Zhang Zhicheng Ma Zhigang Yan',\n",
       "  'metadata': {'page_number': 21, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': 'b26a70b2258f661c676aa566003eca4a'},\n",
       " {'text': 'Ziyang Song Zizheng Pan Zhongyu Zhang Within each role, authors are listed alphabetically by the first name. Names marked with * denote individuals who have departed from our team.',\n",
       "  'metadata': {'page_number': 22, 'source': '../data/DeepSeek-R1.pdf'},\n",
       "  'id': '047ef72cd740ef39dfe3c747bba1016a'}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "04a6c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_MODEL = os.getenv(\"GEMINI_MODEL\", \"models/gemini-2.5-flash-lite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fae23df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./store_emb\")\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"rag_collection\", \n",
    "    configuration={\n",
    "        \"hnsw\": {\n",
    "            \"space\": \"cosine\",\n",
    "            \"ef_construction\": 200,\n",
    "            \"max_neighbors\":32\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "071e33b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "def gemini_embedding_batch(texts, batch_size=2):\n",
    "    result_emb = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        result_emb.extend(embeddings.embed_documents(batch))\n",
    "        time.sleep(2)\n",
    "    return result_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d815394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_store(docs):\n",
    "    # chunking \n",
    "    texts = [doc['text'] for doc in docs]\n",
    "    metadatas = [doc['metadata'] for doc in docs]\n",
    "    embs = gemini_embedding_batch(texts)\n",
    "\n",
    "    \n",
    "    ids = [doc['id'] for doc in docs]\n",
    "    \n",
    "    collection.add(\n",
    "        documents=texts,\n",
    "        metadatas=metadatas,\n",
    "        embeddings=embs,\n",
    "        ids=ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "de454ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 62/62 [02:49<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "embed_and_store(chunk_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a87b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34fc124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965e6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ffd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5b79b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b3861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d5b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
