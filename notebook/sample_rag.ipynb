{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "888c2b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_MODEL = os.getenv(\"GEMINI_MODEL\", \"models/gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d242f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=GEMINI_MODEL, api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b21f0b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./store_emb\")\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"rag_collection\", \n",
    "    configuration={\n",
    "        \"hnsw\": {\n",
    "            \"space\": \"cosine\",\n",
    "            \"ef_construction\": 200,\n",
    "            \"max_neighbors\":32\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "863f430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_pdf_text(pdf_path, min_line_length=10):\n",
    "    \"\"\"\n",
    "    Extracts text from each page, removes headers and footers by detecting repeating lines.\n",
    "    Returns list of dicts: [{ 'page': n, 'text': clean_text }]\n",
    "    \"\"\"\n",
    "    pdf = pdfplumber.open(pdf_path)\n",
    "    pages_text = []\n",
    "    header_footer_candidates = []\n",
    "\n",
    "    # Extract raw lines page-wise\n",
    "    for page_num, page in enumerate(pdf.pages, start=1):\n",
    "        text = page.extract_text() or \"\"\n",
    "        lines = [line.strip() for line in text.split(\"\\n\") if len(line.strip()) > min_line_length]\n",
    "        header_footer_candidates.extend(lines[:2] + lines[-2:])  # top 2 + bottom 2 lines\n",
    "        pages_text.append((page_num, lines))\n",
    "    \n",
    "    # Detect frequently repeating lines (likely headers/footers)\n",
    "    counts = Counter(header_footer_candidates)\n",
    "    repetitive = {line for line, c in counts.items() if c > 0.6 * len(pdf.pages)}\n",
    "\n",
    "    # Clean pages\n",
    "    cleaned_pages = []\n",
    "    for page_num, lines in pages_text:\n",
    "        clean_lines = [ln for ln in lines if ln not in repetitive]\n",
    "        clean_text = \"\\n\".join(clean_lines)\n",
    "        cleaned_pages.append({\"page\": page_num, \"text\": clean_text})\n",
    "    pdf.close()\n",
    "    return cleaned_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c4a5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def build_docs_from_pdf(pdf_path):\n",
    "    cleaned_pages = clean_pdf_text(pdf_path)\n",
    "    file_name = Path(pdf_path).stem\n",
    "    docs = []\n",
    "    for page_obj in cleaned_pages:\n",
    "        page_num = page_obj[\"page\"]\n",
    "        text = page_obj[\"text\"]\n",
    "        metadata = {\n",
    "            \"source\": file_name,\n",
    "            \"page_number\": page_num,\n",
    "            \"file_path\": str(pdf_path)\n",
    "        }\n",
    "        docs.append({\"page_content\": text, \"metadata\": metadata})\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8316970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"../data/\"\n",
    "\n",
    "all_pdfs = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2edacb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'H1' is an invalid float value\n"
     ]
    }
   ],
   "source": [
    "text_documents = []\n",
    "\n",
    "for pdf_file in [all_pdfs[-1]]:\n",
    "    docs = build_docs_from_pdf(pdf_file)\n",
    "    text_documents.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b60539ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_content': 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\\nReinforcement Learning\\nDeepSeek-AI\\nresearch@deepseek.com\\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\\nDeepSeek-R1-Zero,amodeltrainedvialarge-scalereinforcementlearning(RL)withoutsuper-\\nvisedfine-tuning(SFT)asapreliminarystep,demonstratesremarkablereasoningcapabilities.\\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\\nreasoningbehaviors. However,itencounterschallengessuchaspoorreadability,andlanguage\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1,whichincorporatesmulti-stagetrainingandcold-startdatabeforeRL.DeepSeek-\\nR1achievesperformancecomparabletoOpenAI-o1-1217onreasoningtasks. Tosupportthe\\nresearchcommunity,weopen-sourceDeepSeek-R1-Zero,DeepSeek-R1,andsixdensemodels\\n(1.5B,7B,8B,14B,32B,70B)distilledfromDeepSeek-R1basedonQwenandLlama.\\nAIME 2024 Codeforces GPQA Diamond MATH-500 MMLU SWE-bench Verified\\n(Pass@1) (Percentile) (Pass@1) (Pass@1) (Pass@1) (Resolved)\\nDeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\\n96.396.6 97.396.4\\n90.6 90.090.2 90.891.8\\n62.1 58.7 60.059.1\\nFigure1 | BenchmarkperformanceofDeepSeek-R1.\\n1v84921.1052:viXra',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 1,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': '1 Introduction 3\\n1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.2 SummaryofEvaluationResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n2 Approach 5\\n2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 DeepSeek-R1-Zero: ReinforcementLearningontheBaseModel . . . . . . . . . . 5\\n2.2.1 ReinforcementLearningAlgorithm . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2.2 RewardModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2.3 TrainingTemplate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2.4 Performance,Self-evolutionProcessandAhaMomentofDeepSeek-R1-Zero 6\\n2.3 DeepSeek-R1: ReinforcementLearningwithColdStart . . . . . . . . . . . . . . . 9\\n2.3.1 ColdStart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.3.2 Reasoning-orientedReinforcementLearning . . . . . . . . . . . . . . . . . 10\\n2.3.3 RejectionSamplingandSupervisedFine-Tuning . . . . . . . . . . . . . . . 10\\n2.3.4 ReinforcementLearningforallScenarios . . . . . . . . . . . . . . . . . . . 11\\n2.4 Distillation: EmpowerSmallModelswithReasoningCapability . . . . . . . . . . 11\\n3 Experiment 11\\n3.1 DeepSeek-R1Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.2 DistilledModelEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4 Discussion 14\\n4.1 Distillationv.s. ReinforcementLearning . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4.2 UnsuccessfulAttempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5 Conclusion,Limitations,andFutureWork 16\\nA ContributionsandAcknowledgments 20',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 2,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': '1. Introduction\\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\\nevolution(Anthropic,2024;Google,2024;OpenAI,2024a),progressivelydiminishingthegap\\ntowardsArtificialGeneralIntelligence(AGI).\\nRecently,post-traininghasemergedasanimportantcomponentofthefulltrainingpipeline.\\nIthasbeenshowntoenhanceaccuracyonreasoningtasks,alignwithsocialvalues,andadapt\\nto user preferences, all while requiring relatively minimal computational resources against\\npre-training. Inthecontextofreasoningcapabilities,OpenAIâ€™so1(OpenAI,2024b)seriesmodels\\nwere the first to introduce inference-time scaling by increasing the length of the Chain-of-\\nThoughtreasoningprocess. Thisapproachhasachievedsignificantimprovementsinvarious\\nreasoningtasks,suchasmathematics,coding,andscientificreasoning. However,thechallenge\\nofeffectivetest-timescalingremainsanopenquestionfortheresearchcommunity. Severalprior\\nworkshaveexploredvariousapproaches,includingprocess-basedrewardmodels(Lightman\\netal.,2023;Uesatoetal.,2022;Wangetal.,2023),reinforcementlearning(Kumaretal.,2024),\\nandsearchalgorithmssuchasMonteCarloTreeSearchandBeamSearch(Fengetal.,2024;Trinh\\netal.,2024;Xinetal.,2024). However,noneofthesemethodshasachievedgeneralreasoning\\nperformancecomparabletoOpenAIâ€™so1seriesmodels.\\nInthispaper,wetakethefirststeptowardimprovinglanguagemodelreasoningcapabilities\\nusingpurereinforcementlearning(RL).OurgoalistoexplorethepotentialofLLMstodevelop\\nreasoningcapabilitieswithoutanysuperviseddata,focusingontheirself-evolutionthrough\\na pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ\\nGRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.\\nDuringtraining,DeepSeek-R1-Zeronaturallyemergedwithnumerouspowerfulandinteresting\\nreasoningbehaviors. AfterthousandsofRLsteps,DeepSeek-R1-Zeroexhibitssuperperformance\\nonreasoningbenchmarks. Forinstance,thepass@1scoreonAIME2024increasesfrom15.6%to\\n71.0%,andwithmajorityvoting,thescorefurtherimprovesto86.7%,matchingtheperformance\\nofOpenAI-o1-0912.\\nHowever,DeepSeek-R1-Zeroencounterschallengessuchaspoorreadability,andlanguage\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1,whichincorporatesasmallamountofcold-startdataandamulti-stagetraining\\npipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the\\nDeepSeek-V3-Basemodel. Followingthis,weperformreasoning-orientedRLlikeDeepSeek-R1-\\nZero. UponnearingconvergenceintheRLprocess,wecreatenewSFTdatathroughrejection\\nsamplingontheRLcheckpoint,combinedwithsuperviseddatafromDeepSeek-V3indomains\\nsuchaswriting,factualQA,andself-cognition,andthenretraintheDeepSeek-V3-Basemodel.\\nAfterfine-tuningwiththenewdata,thecheckpointundergoesanadditionalRLprocess,taking\\nintoaccountpromptsfromallscenarios. Afterthesesteps,weobtainedacheckpointreferredto\\nasDeepSeek-R1,whichachievesperformanceonparwithOpenAI-o1-1217.\\nWefurtherexploredistillationfromDeepSeek-R1tosmallerdensemodels. UsingQwen2.5-\\n32B(Qwen,2024b)asthebasemodel,directdistillationfromDeepSeek-R1outperformsapplying\\nRLonit. Thisdemonstratesthatthereasoningpatternsdiscoveredbylargerbasemodelsarecru-\\ncialforimprovingreasoningcapabilities. Weopen-sourcethedistilledQwenandLlama(Dubey\\netal.,2024)series. Notably,ourdistilled14Bmodeloutperformsstate-of-the-artopen-source\\nQwQ-32B-Preview(Qwen,2024a)byalargemargin,andthedistilled32Band70Bmodelsseta\\nnewrecordonthereasoningbenchmarksamongdensemodels.',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 3,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': '1.1. Contributions\\nPost-Training: Large-ScaleReinforcementLearningontheBaseModel\\nâ€¢ WedirectlyapplyRLtothebasemodelwithoutrelyingonsupervisedfine-tuning(SFT)as\\napreliminarystep. Thisapproachallowsthemodeltoexplorechain-of-thought(CoT)for\\nsolvingcomplexproblems,resultinginthedevelopmentofDeepSeek-R1-Zero. DeepSeek-\\nR1-Zero demonstrates capabilities such as self-verification, reflection, and generating\\nlongCoTs,markingasignificantmilestonefortheresearchcommunity. Notably,itisthe\\nfirst open research to validate that reasoning capabilities of LLMs can be incentivized\\npurelythroughRL,withouttheneedforSFT.Thisbreakthroughpavesthewayforfuture\\nadvancementsinthisarea.\\nâ€¢ We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL\\nstagesaimedatdiscoveringimprovedreasoningpatternsandaligningwithhumanpref-\\nerences, as well as two SFT stages that serve as the seed for the modelâ€™s reasoning and\\nnon-reasoningcapabilities. Webelievethepipelinewillbenefittheindustrybycreating\\nbettermodels.\\nDistillation: SmallerModelsCanBePowerfulToo\\nâ€¢ Wedemonstratethatthereasoningpatternsoflargermodelscanbedistilledintosmaller\\nmodels,resultinginbetterperformancecomparedtothereasoningpatternsdiscovered\\nthroughRLonsmallmodels. TheopensourceDeepSeek-R1,aswellasitsAPI,willbenefit\\ntheresearchcommunitytodistillbettersmallermodelsinthefuture.\\nâ€¢ UsingthereasoningdatageneratedbyDeepSeek-R1,wefine-tunedseveraldensemodels\\nthatarewidelyusedintheresearchcommunity. Theevaluationresultsdemonstratethat\\nthedistilledsmallerdensemodelsperformexceptionallywellonbenchmarks. DeepSeek-\\nR1-Distill-Qwen-7Bachieves55.5%onAIME2024,surpassingQwQ-32B-Preview. Addi-\\ntionally,DeepSeek-R1-Distill-Qwen-32Bscores72.6%onAIME2024,94.3%onMATH-500,\\nand 57.2% on LiveCodeBench. These results significantly outperform previous open-\\nsourcemodelsandarecomparabletoo1-mini. Weopen-sourcedistilled1.5B,7B,8B,14B,\\n32B,and70BcheckpointsbasedonQwen2.5andLlama3seriestothecommunity.\\n1.2. SummaryofEvaluationResults\\nâ€¢ Reasoningtasks: (1)DeepSeek-R1achievesascoreof79.8%Pass@1onAIME2024,slightly\\nsurpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,\\nperformingonparwithOpenAI-o1-1217andsignificantlyoutperformingothermodels. (2)\\nOncoding-relatedtasks,DeepSeek-R1demonstratesexpertlevelincodecompetitiontasks,\\nasitachieves2,029EloratingonCodeforcesoutperforming96.3%humanparticipantsin\\nthecompetition. Forengineering-relatedtasks,DeepSeek-R1performsslightlybetterthan\\nDeepSeek-V3,whichcouldhelpdevelopersinrealworldtasks.\\nâ€¢ Knowledge: OnbenchmarkssuchasMMLU,MMLU-Pro,andGPQADiamond,DeepSeek-\\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\\nperformanceisslightlybelowthatofOpenAI-o1-1217onthesebenchmarks,DeepSeek-R1\\nsurpassesotherclosed-sourcemodels,demonstratingitscompetitiveedgeineducational\\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstratingitscapabilityinhandlingfact-basedqueries. Asimilartrendisobserved\\nwhereOpenAI-o1surpasses4oonthisbenchmark.',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 4,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'â€¢ Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\\ngeneralquestionanswering,editing,summarization,andmore. Itachievesanimpressive\\nlength-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\\nnaHard,showcasingitsstrongabilitytointelligentlyhandlenon-exam-orientedqueries.\\nAdditionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\\nlong-contextunderstanding,substantiallyoutperformingDeepSeek-V3onlong-context\\nbenchmarks.\\n2. Approach\\n2.1. Overview\\nPrevious work has heavily relied on large amounts of supervised data to enhance model\\nperformance. In this study, we demonstrate that reasoning capabilities can be significantly\\nimproved through large-scale reinforcement learning (RL), even without using supervised\\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and\\n(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\\nlongChain-of-Thought(CoT)examples. 3)DistillthereasoningcapabilityfromDeepSeek-R1to\\nsmalldensemodels.\\n2.2. DeepSeek-R1-Zero: ReinforcementLearningontheBaseModel\\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev-\\nidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works\\nheavilydependedonsuperviseddata,whicharetime-intensivetogather. Inthissection,we\\nexplorethepotentialofLLMstodevelopreasoningcapabilitieswithoutanysuperviseddata,\\nfocusingontheirself-evolutionthroughapurereinforcementlearningprocess. Westartwitha\\nbriefoverviewofourRLalgorithm,followedbythepresentationofsomeexcitingresults,and\\nhopethisprovidesthecommunitywithvaluableinsights.\\n2.2.1. ReinforcementLearningAlgorithm\\nGroupRelativePolicyOptimization InordertosavethetrainingcostsofRL,weadoptGroup\\nRelativePolicyOptimization(GRPO)(Shaoetal.,2024),whichforegoesthecriticmodelthatis\\ntypicallythesamesizeasthepolicymodel,andestimatesthebaselinefromgroupscoresinstead.\\nSpecifically,foreachquestionğ‘,GRPOsamplesagroupofoutputs {ğ‘œ 1 ,ğ‘œ 2 ,Â·Â·Â· ,ğ‘œ ğº } fromtheold\\npolicyğœ‹ ğœƒ andthenoptimizesthepolicymodelğœ‹ ğœƒ bymaximizingthefollowingobjective:\\n(ğœƒ) = E[ğ‘ âˆ¼ ğ‘ƒ(ğ‘„),{ğ‘œ\\n1 âˆ‘ï¸ ğº (cid:18) min (cid:18) ğœ‹ ğœƒ (ğ‘œ ğ‘– |ğ‘) ğ´ ğ‘–,clip (cid:18) ğœ‹ ğœƒ (ğ‘œ ğ‘– |ğ‘) ,1âˆ’ğœ€,1+ğœ€ (cid:19) ğ´ ğ‘– (cid:19) âˆ’ğ›½D ğ¾ğ¿ (cid:0)ğœ‹ ğœƒ ||ğœ‹ ğ‘Ÿğ‘’ğ‘“ (cid:1) (cid:19) , (1)\\nğº ğœ‹ (ğ‘œ |ğ‘) ğœ‹ (ğ‘œ |ğ‘)\\nğ‘–=1 ğ‘œğ‘™ğ‘‘ ğ‘œğ‘™ğ‘‘\\nğœ‹ (ğ‘œ |ğ‘) ğœ‹ (ğ‘œ |ğ‘)\\nD ğ¾ğ¿ (cid:0)ğœ‹ ğœƒ ||ğœ‹ ğ‘Ÿğ‘’ğ‘“ (cid:1) = ğ‘Ÿğ‘’ğ‘“ ğ‘– âˆ’log ğ‘Ÿğ‘’ğ‘“ ğ‘– âˆ’1, (2)\\nğœ‹ (ğ‘œ |ğ‘) ğœ‹ (ğ‘œ |ğ‘)\\nwhere ğœ€ and ğ›½ are hyper-parameters, and ğ´ ğ‘– is the advantage, computed using a group of\\n}correspondingtotheoutputswithineachgroup:',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 5,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'AconversationbetweenUserandAssistant. Theuserasksaquestion,andtheAssistantsolvesit.\\nTheassistantfirstthinksaboutthereasoningprocessinthemindandthenprovidestheuser\\nwiththeanswer. Thereasoningprocessandanswerareenclosedwithin<think></think>and\\n<answer></answer>tags,respectively,i.e.,<think>reasoningprocesshere</think>\\n<answer>answerhere</answer>. User: prompt. Assistant:\\nTable1 | TemplateforDeepSeek-R1-Zero. promptwillbereplacedwiththespecificreasoning\\nquestionduringtraining.\\n2.2.2. RewardModeling\\nTherewardisthesourceofthetrainingsignal,whichdecidestheoptimizationdirectionofRL.\\nTotrainDeepSeek-R1-Zero,weadoptarule-basedrewardsystemthatmainlyconsistsoftwo\\ntypesofrewards:\\nâ€¢ Accuracyrewards: Theaccuracyrewardmodelevaluateswhethertheresponseiscorrect.\\nForexample,inthecaseofmathproblemswithdeterministicresults,themodelisrequired\\nto provide the final answer in a specified format (e.g., within a box), enabling reliable\\nrule-basedverificationofcorrectness. Similarly,forLeetCodeproblems,acompilercanbe\\nusedtogeneratefeedbackbasedonpredefinedtestcases.\\nâ€¢ Formatrewards: Inadditiontotheaccuracyrewardmodel,weemployaformatreward\\nmodelthatenforcesthemodeltoputitsthinkingprocessbetweenâ€˜<think>â€™andâ€˜</think>â€™\\nWedonotapplytheoutcomeorprocessneuralrewardmodelindevelopingDeepSeek-R1-Zero,\\nbecausewefindthattheneuralrewardmodelmaysufferfromrewardhackinginthelarge-scale\\nreinforcement learning process, and retraining the reward model needs additional training\\nresourcesanditcomplicatesthewholetrainingpipeline.\\n2.2.3. TrainingTemplate\\nTo train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides\\nthe base model to adhere to our specified instructions. As depicted in Table 1, this template\\nrequiresDeepSeek-R1-Zerotofirstproduceareasoningprocess,followedbythefinalanswer.\\nWeintentionallylimitourconstraintstothisstructuralformat,avoidinganycontent-specific\\nbiasesâ€”suchasmandatingreflectivereasoningorpromotingparticularproblem-solvingstrate-\\ngiesâ€”toensurethatwecanaccuratelyobservethemodelâ€™snaturalprogressionduringtheRL\\n2.2.4. Performance,Self-evolutionProcessandAhaMomentofDeepSeek-R1-Zero\\nPerformanceofDeepSeek-R1-Zero Figure2depictstheperformancetrajectoryofDeepSeek-\\nR1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,\\nDeepSeek-R1-Zerodemonstratesasteadyandconsistentenhancementinperformanceasthe\\nRL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant\\nincrease, jumpingfromaninitial15.6%toanimpressive71.0%, reachingperformancelevels\\ncomparabletoOpenAI-o1-0912. ThissignificantimprovementhighlightstheefficacyofourRL\\nalgorithminoptimizingthemodelâ€™sperformanceovertime.\\nTable2providesacomparativeanalysisbetweenDeepSeek-R1-ZeroandOpenAIâ€™so1-0912\\nmodelsacrossavarietyofreasoning-relatedbenchmarks. ThefindingsrevealthatRLempowers',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 6,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'GPQA LiveCode\\nAIME2024 MATH-500 CodeForces\\nModel Diamond Bench\\npass@1 cons@64 pass@1 pass@1 pass@1 rating\\nOpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820\\nOpenAI-o1-0912 74.4 83.3 94.8 77.3 63.4 1843\\nDeepSeek-R1-Zero 71.0 86.7 95.9 73.3 50.0 1444\\nTable2 | ComparisonofDeepSeek-R1-ZeroandOpenAIo1modelsonreasoning-related\\nbenchmarks.\\nFigure2 | AIMEaccuracyofDeepSeek-R1-Zeroduringtraining. Foreachquestion,wesample\\n16responsesandcalculatetheoverallaverageaccuracytoensureastableevaluation.\\nDeepSeek-R1-Zerotoattainrobustreasoningcapabilitieswithouttheneedforanysupervised\\nfine-tuning data. This is a noteworthy achievement, as it underscores the modelâ€™s ability to\\nlearnandgeneralizeeffectivelythroughRLalone. Additionally,theperformanceofDeepSeek-\\nR1-Zero can be further augmented through the application of majority voting. For example,\\nwhenmajorityvotingisemployedontheAIMEbenchmark,DeepSeek-R1-Zeroâ€™sperformance\\nescalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The\\nabilityofDeepSeek-R1-Zerotoachievesuchcompetitiveperformance,bothwithandwithout\\nmajority voting, highlights its strong foundational capabilities and its potential for further\\nadvancementsinreasoningtasks.\\nSelf-evolutionProcessofDeepSeek-R1-Zero Theself-evolutionprocessofDeepSeek-R1-Zero\\nisafascinatingdemonstrationofhowRLcandriveamodeltoimproveitsreasoningcapabilities\\nautonomously. ByinitiatingRLdirectlyfromthebasemodel,wecancloselymonitorthemodelâ€™s\\nprogressionwithouttheinfluenceofthesupervisedfine-tuningstage. Thisapproachprovides\\naclearviewofhowthemodelevolvesovertime,particularlyintermsofitsabilitytohandle\\ncomplexreasoningtasks.\\nAsdepictedinFigure3,thethinkingtimeofDeepSeek-R1-Zeroshowsconsistentimprove-',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 7,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'Figure3 | TheaverageresponselengthofDeepSeek-R1-ZeroonthetrainingsetduringtheRL\\nprocess. DeepSeek-R1-Zeronaturallylearnstosolvereasoningtaskswithmorethinkingtime.\\nmentthroughoutthetrainingprocess. Thisimprovementisnottheresultofexternaladjustments\\nbutratheranintrinsicdevelopmentwithinthemodel. DeepSeek-R1-Zeronaturallyacquiresthe\\nabilitytosolveincreasinglycomplexreasoningtasksbyleveragingextendedtest-timecompu-\\ntation. Thiscomputationrangesfromgeneratinghundredstothousandsofreasoningtokens,\\nallowingthemodeltoexploreandrefineitsthoughtprocessesingreaterdepth.\\nOneofthemostremarkableaspectsofthisself-evolutionistheemergenceofsophisticated\\nbehaviorsasthetest-timecomputationincreases. Behaviorssuchasreflectionâ€”wherethemodel\\nrevisits and reevaluates its previous stepsâ€”and the exploration of alternative approaches to\\nproblem-solvingarisespontaneously. Thesebehaviorsarenotexplicitlyprogrammedbutinstead\\nemergeasaresultofthemodelâ€™sinteractionwiththereinforcementlearningenvironment. This\\nspontaneousdevelopmentsignificantlyenhancesDeepSeek-R1-Zeroâ€™sreasoningcapabilities,\\nenablingittotacklemorechallengingtaskswithgreaterefficiencyandaccuracy.\\nAhaMomentofDeepSeek-R1-Zero Aparticularlyintriguingphenomenonobservedduring\\nthe training of DeepSeek-R1-Zero is the occurrence of an â€œaha momentâ€. This moment, as\\nillustrated in Table 3, occurs in an intermediate version of the model. During this phase,\\nDeepSeek-R1-Zerolearnstoallocatemorethinkingtimetoaproblembyreevaluatingitsinitial\\napproach. This behavior is not only a testament to the modelâ€™s growing reasoning abilities\\nbut also a captivating example of how reinforcement learning can lead to unexpected and\\nsophisticatedoutcomes.\\nThis moment is not only an â€œaha momentâ€ for the model but also for the researchers\\nobservingitsbehavior. Itunderscoresthepowerandbeautyofreinforcementlearning: rather\\nthanexplicitlyteachingthemodelonhowtosolveaproblem,wesimplyprovideitwiththe\\nright incentives, and it autonomously develops advanced problem-solving strategies. The\\nâ€œaha momentâ€ serves as a powerful reminder of the potential of RL to unlock new levels of\\nintelligenceinartificialsystems,pavingthewayformoreautonomousandadaptivemodelsin',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 8,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'Question: Ifğ‘ > 1,thenthesumoftherealsolutionsof ğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥ isequalto\\nResponse: <think>\\nTosolvetheequation ğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥,letâ€™sstartbysquaringbothÂ·Â·Â·\\n(cid:16)âˆšï¸ âˆš (cid:17)2 âˆš\\nğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥2 =â‡’ ğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥2.\\nRearrangetoisolatetheinnersquarerootterm:\\n(ğ‘âˆ’ğ‘¥2)2 = ğ‘+ğ‘¥ =â‡’ ğ‘2âˆ’2ğ‘ğ‘¥2+(ğ‘¥2)2 = ğ‘+ğ‘¥ =â‡’ ğ‘¥4âˆ’2ğ‘ğ‘¥2âˆ’ğ‘¥+(ğ‘2âˆ’ğ‘) = 0\\nWait,wait. Wait. Thatâ€™sanahamomentIcanflaghere.\\nLetâ€™sreevaluatethisstep-by-steptoidentifyifthecorrectsumcanbeÂ·Â·Â·\\nWestartedwiththeequation:\\nFirst,letâ€™ssquarebothsides:\\nğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥2 =â‡’ ğ‘+ğ‘¥ = ğ‘âˆ’ğ‘¥2\\nNext,Icouldsquarebothsidesagain,treatingtheequation: Â·Â·Â·\\nTable3 | Aninterestingâ€œahamomentâ€ofanintermediateversionofDeepSeek-R1-Zero. The\\nmodellearnstorethinkusingananthropomorphictone. Thisisalsoanahamomentforus,\\nallowingustowitnessthepowerandbeautyofreinforcementlearning.\\nDrawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning\\ncapabilitiesandautonomouslydevelopsunexpectedandpowerfulreasoningbehaviors,itfaces\\nseveralissues. Forinstance,DeepSeek-R1-Zerostruggleswithchallengeslikepoorreadability,\\nand language mixing. To make reasoning processes more readable and share them with the\\nopen community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly\\ncold-startdata.\\n2.3. DeepSeek-R1: ReinforcementLearningwithColdStart\\nInspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can\\nreasoningperformancebefurtherimprovedorconvergenceacceleratedbyincorporatingasmall\\namount of high-quality data as a cold start? 2) How can we train a user-friendly model that\\nnotonlyproducesclearandcoherentChainsofThought(CoT)butalsodemonstratesstrong\\ngeneralcapabilities? Toaddressthesequestions,wedesignapipelinetotrainDeepSeek-R1. The\\npipelineconsistsoffourstages,outlinedasfollows.\\n2.3.1. ColdStart\\nUnlikeDeepSeek-R1-Zero,topreventtheearlyunstablecoldstartphaseofRLtrainingfrom\\nthe base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data\\nto fine-tune the model as the initial RL actor. To collect such data, we have explored several\\napproaches: using few-shot prompting with a long CoT as an example, directly prompting\\nmodelstogeneratedetailedanswerswithreflectionandverification,gatheringDeepSeek-R1-\\nZerooutputsinareadableformat,andrefiningtheresultsthroughpost-processingbyhuman\\nannotators.\\nInthiswork,wecollectthousandsofcold-startdatatofine-tunetheDeepSeek-V3-Baseas\\nthe starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 9,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'â€¢ Readability: AkeylimitationofDeepSeek-R1-Zeroisthatitscontentisoftennotsuitable\\nfor reading. Responses may mix multiple languages or lack markdown formatting to\\nhighlightanswersforusers. Incontrast,whencreatingcold-startdataforDeepSeek-R1,\\nwedesignareadablepatternthatincludesasummaryattheendofeachresponseand\\nfilters out responses that are not reader-friendly. Here, we define the output format as\\n|special_token|<reasoning_process>|special_token|<summary>,wherethereasoning\\nprocessistheCoTforthequery,andthesummaryisusedtosummarizethereasoning\\nâ€¢ Potential: Bycarefullydesigningthepatternforcold-startdatawithhumanpriors, we\\nobservebetterperformanceagainstDeepSeek-R1-Zero. Webelievetheiterativetrainingis\\nabetterwayforreasoningmodels.\\n2.3.2. Reasoning-orientedReinforcementLearning\\nAfter fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale\\nreinforcementlearningtrainingprocessasemployedinDeepSeek-R1-Zero. Thisphasefocuses\\nonenhancingthemodelâ€™sreasoningcapabilities,particularlyinreasoning-intensivetaskssuch\\nascoding,mathematics,science,andlogicreasoning,whichinvolvewell-definedproblemswith\\nclearsolutions. Duringthetrainingprocess,weobservethatCoToftenexhibitslanguagemixing,\\nparticularlywhenRLpromptsinvolvemultiplelanguages. Tomitigatetheissueoflanguage\\nmixing,weintroducealanguageconsistencyrewardduringRLtraining,whichiscalculated\\nastheproportionoftargetlanguagewordsintheCoT.Althoughablationexperimentsshow\\nthat such alignment results in a slight degradation in the modelâ€™s performance, this reward\\nalignswithhumanpreferences,makingitmorereadable. Finally,wecombinetheaccuracyof\\nreasoningtasksandtherewardforlanguageconsistencybydirectlysummingthemtoformthe\\nfinalreward. WethenapplyRLtrainingonthefine-tunedmodeluntilitachievesconvergence\\nonreasoningtasks.\\n2.3.3. RejectionSamplingandSupervisedFine-Tuning\\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT\\n(SupervisedFine-Tuning)dataforthesubsequentround. Unliketheinitialcold-startdata,which\\nprimarilyfocusesonreasoning,thisstageincorporatesdatafromotherdomainstoenhancethe\\nmodelâ€™scapabilitiesinwriting,role-playing,andothergeneral-purposetasks. Specifically,we\\ngeneratethedataandfine-tunethemodelasdescribedbelow.\\nReasoningdata Wecuratereasoningpromptsandgeneratereasoningtrajectoriesbyperform-\\ningrejectionsamplingfromthecheckpointfromtheaboveRLtraining. Inthepreviousstage,\\nweonlyincludeddatathatcouldbeevaluatedusingrule-basedrewards. However,inthisstage,\\nweexpandthedatasetbyincorporatingadditionaldata,someofwhichuseagenerativereward\\nmodel by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.\\nAdditionally, because the model output is sometimes chaotic and difficult to read, we have\\nfiltered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For\\neachprompt,wesamplemultipleresponsesandretainonlythecorrectones. Intotal,wecollect\\nabout600kreasoningrelatedtrainingsamples.',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 10,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition,\\nandtranslation,weadopttheDeepSeek-V3pipelineandreuseportionsoftheSFTdatasetof\\nDeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential\\nchain-of-thoughtbeforeansweringthequestionbyprompting. However,forsimplerqueries,\\nsuch as â€œhelloâ€ we do not provide a CoT in response. In the end, we collected a total of\\napproximately200ktrainingsamplesthatareunrelatedtoreasoning.\\nWefine-tuneDeepSeek-V3-Basefortwoepochsusingtheabovecurateddatasetofabout\\n800ksamples.\\n2.3.4. ReinforcementLearningforallScenarios\\nTofurtheralignthemodelwithhumanpreferences,weimplementasecondaryreinforcement\\nlearningstageaimedatimprovingthemodelâ€™shelpfulnessandharmlessnesswhilesimultane-\\nouslyrefiningitsreasoningcapabilities. Specifically,wetrainthemodelusingacombination\\nof reward signals and diverse prompt distributions. For reasoning data, we adhere to the\\nmethodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the\\nlearningprocessinmath,code,andlogicalreasoningdomains. Forgeneraldata,weresortto\\nreward models to capture human preferences in complex and nuanced scenarios. We build\\nupontheDeepSeek-V3pipelineandadoptasimilardistributionofpreferencepairsandtrain-\\ning prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the\\nassessmentemphasizestheutilityandrelevanceoftheresponsetotheuserwhileminimizing\\ninterferencewiththeunderlyingreasoningprocess. Forharmlessness,weevaluatetheentire\\nresponseofthemodel,includingboththereasoningprocessandthesummary,toidentifyand\\nmitigate any potential risks, biases, or harmful content that may arise during the generation\\nprocess. Ultimately,theintegrationofrewardsignalsanddiversedatadistributionsenablesus\\ntotrainamodelthatexcelsinreasoningwhileprioritizinghelpfulnessandharmlessness.\\n2.4. Distillation: EmpowerSmallModelswithReasoningCapability\\nToequipmoreefficientsmallermodelswithreasoningcapabilitieslikeDeepSeek-R1,wedirectly\\nfine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using\\nthe800ksamplescuratedwithDeepSeek-R1,asdetailedinÂ§2.3.3. Ourfindingsindicatethat\\nthisstraightforwarddistillationmethodsignificantlyenhancesthereasoningabilitiesofsmaller\\nmodels. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-\\n14B,Qwen2.5-32B,Llama-3.1-8B,andLlama-3.3-70B-Instruct. WeselectLlama-3.3becauseits\\nreasoningcapabilityisslightlybetterthanthatofLlama-3.1.\\nFor distilled models, we apply only SFT and do not include an RL stage, even though\\nincorporating RL could substantially boost model performance. Our primary goal here is to\\ndemonstrate the effectiveness of the distillation technique, leaving the exploration of the RL\\nstagetothebroaderresearchcommunity.\\n3. Experiment\\nBenchmarks WeevaluatemodelsonMMLU(Hendrycksetal.,2020),MMLU-Redux(Gema\\netal.,2024),MMLU-Pro(Wangetal.,2024),C-Eval(Huangetal.,2023),andCMMLU(Lietal.,\\n2023),IFEval(Zhouetal.,2023),FRAMES(Krishnaetal.,2024),GPQADiamond (Reinetal.,\\n2023),SimpleQA(OpenAI,2024c),C-SimpleQA(Heetal.,2024),SWE-BenchVerified(OpenAI,',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 11,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': '2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 â€“ 2025-01), Codeforces 2, Chinese\\nNationalHighSchoolMathematicsOlympiad(CNMO2024)3,andAmericanInvitationalMath-\\nematicsExamination2024(AIME2024)(MAA,2024). Inadditiontostandardbenchmarks,we\\nalsoevaluateourmodelsonopen-endedgenerationtasksusingLLMsasjudges. Specifically,we\\nadheretotheoriginalconfigurationsofAlpacaEval2.0(Duboisetal.,2024)andArena-Hard(Li\\netal.,2024),whichleverageGPT-4-Turbo-1106asjudgesforpairwisecomparisons. Here,we\\nonly feed the final summary to evaluation to avoid the length bias. For distilled models, we\\nreport representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and\\nLiveCodeBench.\\nEvaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as\\nMMLU,DROP,GPQADiamond,andSimpleQAareevaluatedusingpromptsfromthesimple-\\nevalsframework. ForMMLU-Redux,weadopttheZero-Evalpromptformat(Lin,2024)ina\\nzero-shotsetting. IntermsofMMLU-Pro,C-EvalandCLUE-WSC,sincetheoriginalprompts\\nare few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot\\nmay hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation\\nprotocolswithdefaultpromptsprovidedbytheircreators. Forcodeandmathbenchmarks,the\\nHumanEval-Muldatasetcoverseightmainstreamprogramminglanguages(Python,Java,C++,\\nC#,JavaScript,TypeScript,PHP,andBash). ModelperformanceonLiveCodeBenchisevaluated\\nusingCoTformat,withdatacollectedbetweenAugust2024andJanuary2025. TheCodeforces\\ndatasetisevaluatedusingproblemsfrom10Div.2contestsalongwithexpert-craftedtestcases,\\nafter which the expected ratings and percentages of competitors are calculated. SWE-Bench\\nverified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related\\nbenchmarksaremeasuredusinga\"diff\"format. DeepSeek-R1outputsarecappedatamaximum\\nof32,768tokensforeachbenchmark.\\nBaselines Weconductcomprehensiveevaluationsagainstseveralstrongbaselines,including\\nDeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, andOpenAI-o1-1217.\\nSinceaccessingtheOpenAI-o1-1217APIischallenginginmainlandChina,wereportitsperfor-\\nmancebasedonofficialreports. Fordistilledmodels,wealsocomparetheopen-sourcemodel\\nQwQ-32B-Preview(Qwen,2024a).\\nEvaluation Setup We set the maximum generation length to 32,768 tokens for the models.\\nWe found that using greedy decoding to evaluate long-output reasoning models results in\\nhigherrepetitionratesandsignificantvariabilityacrossdifferentcheckpoints. Therefore,we\\ndefaulttopass@ğ‘˜evaluation(Chenetal.,2021)andreportpass@1usinganon-zerotemperature.\\nSpecifically, we use a sampling temperature of 0.6 and a top-ğ‘ value of 0.95 to generate ğ‘˜\\nresponses(typicallybetween4and64,dependingonthetestsetsize)foreachquestion. Pass@1\\nisthencalculatedas\\npass@1 = ğ‘ ğ‘–,\\nwhere ğ‘ ğ‘– denotes the correctness of the ğ‘–-th response. This method provides more reliable\\nperformanceestimates. ForAIME2024,wealsoreportconsensus(majorityvote)results(Wang\\netal.,2022)using64samples,denotedascons@64.\\n1https://aider.chat\\n2https://codeforces.com\\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 12,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': '3.1. DeepSeek-R1Evaluation\\nClaude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\\nBenchmark(Metric)\\nSonnet-1022 0513 V3 o1-mini o1-1217 R1\\nArchitecture - - MoE - - MoE\\n#ActivatedParams - - 37B - - 37B\\n#TotalParams - - 671B - - 671B\\nMMLU(Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8\\nMMLU-Redux(EM) 88.9 88.0 89.1 86.7 - 92.9\\nMMLU-Pro(EM) 78.0 72.6 75.9 80.3 - 84.0\\nDROP(3-shotF1) 88.3 83.7 91.6 83.9 90.2 92.2\\nIF-Eval(PromptStrict) 86.5 84.3 86.1 84.8 - 83.3\\nGPQADiamond(Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5\\nSimpleQA(Correct) 28.4 38.2 24.9 7.0 47.0 30.1\\nFRAMES(Acc.) 72.5 80.5 73.3 76.9 - 82.5\\nAlpacaEval2.0(LC-winrate) 52.0 51.1 70.0 57.8 - 87.6\\nArenaHard(GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3\\nLiveCodeBench(Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9\\nCodeforces(Percentile) 20.3 23.6 58.7 93.4 96.6 96.3\\nCodeforces(Rating) 717 759 1134 1820 2061 2029\\nSWEVerified(Resolved) 50.8 38.8 42.0 41.6 48.9 49.2\\nAider-Polyglot(Acc.) 45.3 16.0 49.6 32.9 61.7 53.3\\nAIME2024(Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8\\nMath MATH-500(Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3\\nCNMO2024(Pass@1) 13.1 10.8 43.2 67.6 - 78.8\\nCLUEWSC(EM) 85.4 87.9 90.9 89.9 - 92.8\\nChinese C-Eval(EM) 76.7 76.0 86.5 68.9 - 91.8\\nC-SimpleQA(Correct) 55.4 58.7 68.0 40.3 - 63.7\\nTable4 | ComparisonbetweenDeepSeek-R1andotherrepresentativemodels.\\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA\\nDiamond,DeepSeek-R1demonstratessuperiorperformancecomparedtoDeepSeek-V3. Thisim-\\nprovementisprimarilyattributedtoenhancedaccuracyinSTEM-relatedquestions,wheresignif-\\nicantgainsareachievedthroughlarge-scalereinforcementlearning. Additionally,DeepSeek-R1\\nexcelsonFRAMES,along-context-dependentQAtask,showcasingitsstrongdocumentanalysis\\ncapabilities. This highlights the potential of reasoning models in AI-driven search and data\\nanalysistasks. OnthefactualbenchmarkSimpleQA,DeepSeek-R1outperformsDeepSeek-V3,\\ndemonstratingitscapabilityinhandlingfact-basedqueries. Asimilartrendisobservedwhere\\nOpenAI-o1surpassesGPT-4oonthisbenchmark. However,DeepSeek-R1performsworsethan\\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\\nansweringcertainqueriesaftersafetyRL.WithoutsafetyRL,DeepSeek-R1couldachievean\\naccuracyofover70%.\\nDeepSeek-R1alsodeliversimpressiveresultsonIF-Eval,abenchmarkdesignedtoassessa\\nmodelâ€™sabilitytofollowformatinstructions. Theseimprovementscanbelinkedtotheinclusion\\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\\ntraining. Furthermore,remarkableperformanceisobservedonAlpacaEval2.0andArenaHard,\\nindicatingDeepSeek-R1â€™sstrengthsinwritingtasksandopen-domainquestionanswering. Its\\nsignificantoutperformanceofDeepSeek-V3underscoresthegeneralizationbenefitsoflarge-scale\\nRL,whichnotonlyboostsreasoningcapabilitiesbutalsoimprovesperformanceacrossdiverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverageof689tokensonArenaHardand2,218charactersonAlpacaEval2.0. Thisindicatesthat',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 13,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'DeepSeek-R1avoidsintroducinglengthbiasduringGPT-basedevaluations,furthersolidifying\\nitsrobustnessacrossmultipletasks.\\nOn math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,\\nsurpassingothermodelsbyalargemargin. Asimilartrendisobservedoncodingalgorithm\\ntasks,suchasLiveCodeBenchandCodeforces,wherereasoning-focusedmodelsdominatethese\\nbenchmarks. Onengineering-orientedcodingtasks,OpenAI-o1-1217outperformsDeepSeek-R1\\nonAiderbutachievescomparableperformanceonSWEVerified. Webelievetheengineering\\nperformance of DeepSeek-R1 will improve in the next version, as the amount of related RL\\ntrainingdatacurrentlyremainsverylimited.\\n3.2. DistilledModelEvaluation\\nGPQA LiveCode\\nAIME2024 MATH-500 CodeForces\\nModel Diamond Bench\\npass@1 cons@64 pass@1 pass@1 pass@1 rating\\nGPT-4o-0513 9.3 13.4 74.6 49.9 32.9 759\\nClaude-3.5-Sonnet-1022 16.0 26.7 78.3 65.0 38.9 717\\nOpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820\\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 1316\\nDeepSeek-R1-Distill-Qwen-1.5B 28.9 52.7 83.9 33.8 16.9 954\\nDeepSeek-R1-Distill-Qwen-7B 55.5 83.3 92.8 49.1 37.6 1189\\nDeepSeek-R1-Distill-Qwen-14B 69.7 80.0 93.9 59.1 53.1 1481\\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 1691\\nDeepSeek-R1-Distill-Llama-8B 50.4 80.0 89.1 49.0 39.6 1205\\nDeepSeek-R1-Distill-Llama-70B 70.0 86.7 94.5 65.2 57.5 1633\\nTable5 | ComparisonofDeepSeek-R1distilledmodelsandothercomparablemodelson\\nreasoning-relatedbenchmarks.\\nAsshowninTable5,simplydistillingDeepSeek-R1â€™soutputsenablestheefficientDeepSeek-\\nR1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-\\nreasoningmodelslikeGPT-4o-0513acrosstheboard. DeepSeek-R1-14BsurpassesQwQ-32B-\\nPreviewonallevaluationmetrics,whileDeepSeek-R1-32BandDeepSeek-R1-70Bsignificantly\\nexceedo1-minionmostbenchmarks. Theseresultsdemonstratethestrongpotentialofdistilla-\\ntion. Additionally,wefoundthatapplyingRLtothesedistilledmodelsyieldssignificantfurther\\ngains. Webelievethiswarrantsfurtherexplorationandthereforepresentonlytheresultsofthe\\nsimpleSFT-distilledmodelshere.\\n4. Discussion\\n4.1. Distillationv.s. ReinforcementLearning\\nInSection3.2,wecanseethatbydistillingDeepSeek-R1,thesmallmodelcanachieveimpressive\\nresults. However,thereisstillonequestionleft: canthemodelachievecomparableperformance\\nthroughthelarge-scaleRLtrainingdiscussedinthepaperwithoutdistillation?\\nToanswerthisquestion,weconductlarge-scaleRLtrainingonQwen-32B-Baseusingmath,\\ncode,andSTEMdata,trainingforover10Ksteps,resultinginDeepSeek-R1-Zero-Qwen-32B.The\\nexperimentalresults,showninTable6,demonstratethatthe32Bbasemodel,afterlarge-scale',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 14,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'AIME2024 MATH-500 GPQADiamond LiveCodeBench\\npass@1 cons@64 pass@1 pass@1 pass@1\\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9\\nDeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2\\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2\\nTable6 | ComparisonofdistilledandRLModelsonReasoning-RelatedBenchmarks.\\nRL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-\\nDistill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than\\nDeepSeek-R1-Zero-Qwen-32Bacrossallbenchmarks.\\nTherefore,wecandrawtwoconclusions: First,distillingmorepowerfulmodelsintosmaller\\nonesyieldsexcellentresults,whereassmallermodelsrelyingonthelarge-scaleRLmentionedin\\nthispaperrequireenormouscomputationalpowerandmaynotevenachievetheperformance\\nofdistillation. Second,whiledistillationstrategiesarebotheconomicalandeffective,advancing\\nbeyondtheboundariesofintelligencemaystillrequiremorepowerfulbasemodelsandlarger-\\nscalereinforcementlearning.\\n4.2. UnsuccessfulAttempts\\nIntheearlystagesofdevelopingDeepSeek-R1,wealsoencounteredfailuresandsetbacksalong\\ntheway. Weshareourfailureexperiencesheretoprovideinsights,butthisdoesnotimplythat\\ntheseapproachesareincapableofdevelopingeffectivereasoningmodels.\\nProcessRewardModel(PRM) PRMisareasonablemethodtoguidethemodeltowardbetter\\napproachesforsolvingreasoningtasks(Lightmanetal.,2023;Uesatoetal.,2022;Wangetal.,\\n2023). However,inpractice,PRMhasthreemainlimitationsthatmayhinderitsultimatesuc-\\ncess. First,itischallengingtoexplicitlydefineafine-grainstepingeneralreasoning. Second,\\ndeterminingwhetherthecurrentintermediatestepiscorrectisachallengingtask. Automated\\nannotationusingmodelsmaynotyieldsatisfactoryresults,whilemanualannotationisnotcon-\\nducivetoscalingup. Third,onceamodel-basedPRMisintroduced,itinevitablyleadstoreward\\nhacking(Gaoetal.,2022),andretrainingtherewardmodelneedsadditionaltrainingresources\\nanditcomplicatesthewholetrainingpipeline. Inconclusion,whilePRMdemonstratesagood\\nabilitytorerankthetop-Nresponsesgeneratedbythemodelorassistinguidedsearch(Snell\\netal.,2024),itsadvantagesarelimitedcomparedtotheadditionalcomputationaloverheadit\\nintroducesduringthelarge-scalereinforcementlearningprocessinourexperiments.\\nMonteCarloTreeSearch(MCTS) InspiredbyAlphaGo(Silveretal.,2017b)andAlphaZero(Sil-\\nver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time\\ncomputescalability. Thisapproachinvolvesbreakinganswersintosmallerpartstoallowthe\\nmodeltoexplorethesolutionspacesystematically. Tofacilitatethis,wepromptthemodelto\\ngeneratemultipletagsthatcorrespondtospecificreasoningstepsnecessaryforthesearch. For\\ntraining,wefirstusecollectedpromptstofindanswersviaMCTSguidedbyapre-trainedvalue\\nmodel. Subsequently,weusetheresultingquestion-answerpairstotrainboththeactormodel\\nandthevaluemodel,iterativelyrefiningtheprocess.\\nHowever,thisapproachencountersseveralchallengeswhenscalingupthetraining. First,\\nunlike chess, wherethe search spaceis relatively well-defined, tokengeneration presents an',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 15,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'exponentiallylargersearchspace. Toaddressthis,wesetamaximumextensionlimitforeach\\nnode, but this can lead to the model getting stuck in local optima. Second, the value model\\ndirectly influences the quality of generation since it guides each step of the search process.\\nTrainingafine-grainedvaluemodelisinherentlydifficult,whichmakesitchallengingforthe\\nmodeltoiterativelyimprove. WhileAlphaGoâ€™scoresuccessreliedontrainingavaluemodelto\\nprogressivelyenhanceitsperformance,thisprincipleprovesdifficulttoreplicateinoursetup\\nduetothecomplexitiesoftokengeneration.\\nInconclusion,whileMCTScanimproveperformanceduringinferencewhenpairedwitha\\npre-trainedvaluemodel,iterativelyboostingmodelperformancethroughself-searchremainsa\\nsignificantchallenge.\\n5. Conclusion, Limitations, and Future Work\\nInthiswork,weshareourjourneyinenhancingmodelreasoningabilitiesthroughreinforcement\\nlearning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start\\ndata, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,\\nleveragingcold-startdataalongsideiterativeRLfine-tuning. Ultimately,DeepSeek-R1achieves\\nperformancecomparabletoOpenAI-o1-1217onarangeoftasks.\\nWe further explore distillation the reasoning capability to small dense models. We use\\nDeepSeek-R1astheteachermodeltogenerate800Ktrainingsamples,andfine-tuneseveralsmall\\ndensemodels. Theresultsarepromising: DeepSeek-R1-Distill-Qwen-1.5BoutperformsGPT-4o\\nandClaude-3.5-Sonnetonmathbenchmarkswith28.9%onAIMEand83.9%onMATH.Other\\ndense models also achieve impressive results, significantly outperforming other instruction-\\ntunedmodelsbasedonthesameunderlyingcheckpoints.\\nInthefuture,weplantoinvestinresearchacrossthefollowingdirectionsforDeepSeek-R1.\\nâ€¢ GeneralCapability: Currently,thecapabilitiesofDeepSeek-R1fallshortofDeepSeek-V3\\nin tasks such as function calling, multi-turn, complex role-playing, and JSON output.\\nMovingforward,weplantoexplorehowlongCoTcanbeleveragedtoenhancetasksin\\nthesefields.\\nâ€¢ LanguageMixing: DeepSeek-R1iscurrentlyoptimizedforChineseandEnglish,which\\nmay result in language mixing issues when handling queries in other languages. For\\ninstance,DeepSeek-R1mightuseEnglishforreasoningandresponses,evenifthequeryis\\ninalanguageotherthanEnglishorChinese. Weaimtoaddressthislimitationinfuture\\nâ€¢ PromptingEngineering: WhenevaluatingDeepSeek-R1,weobservethatitissensitive\\ntoprompts. Few-shotpromptingconsistentlydegradesitsperformance. Therefore,we\\nrecommend users directly describe the problem and specify the output format using a\\nzero-shotsettingforoptimalresults.\\nâ€¢ Software Engineering Tasks: Due to the long evaluation times, which impact the effi-\\nciency of the RL process, large-scale RL has not been applied extensively in software\\nengineeringtasks. Asaresult,DeepSeek-R1hasnotdemonstratedahugeimprovement\\nover DeepSeek-V3 on software engineering benchmarks. Future versions will address\\nthisbyimplementingrejectionsamplingonsoftwareengineeringdataorincorporating\\nasynchronousevaluationsduringtheRLprocesstoimproveefficiency.',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 16,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'AI@Meta. Llama3.1modelcard,2024. URLhttps://github.com/meta-llama/llama-m\\nodels/blob/main/models/llama3_1/MODEL_CARD.md.\\nAnthropic. Claude3.5sonnet,2024. URLhttps://www.anthropic.com/news/claude-3\\nM.Chen,J.Tworek,H.Jun,Q.Yuan,H.P.deOliveiraPinto,J.Kaplan,H.Edwards,Y.Burda,\\nN.Joseph,G.Brockman,A.Ray,R.Puri,G.Krueger,M.Petrov,H.Khlaaf,G.Sastry,P.Mishkin,\\nB.Chan,S.Gray,N.Ryder,M.Pavlov,A.Power,L.Kaiser,M.Bavarian,C.Winter,P.Tillet,\\nF.P.Such,D.Cummings,M.Plappert,F.Chantzis,E.Barnes,A.Herbert-Voss,W.H.Guss,\\nA.Nichol,A.Paino,N.Tezak,J.Tang,I.Babuschkin,S.Balaji,S.Jain,W.Saunders,C.Hesse,\\nA.N.Carr,J.Leike,J.Achiam,V.Misra,E.Morikawa,A.Radford,M.Knight,M.Brundage,\\nM.Murati,K.Mayer,P.Welinder,B.McGrew,D.Amodei,S.McCandlish,I.Sutskever,and\\nW.Zaremba. Evaluatinglargelanguagemodelstrainedoncode. CoRR,abs/2107.03374,2021.\\nURLhttps://arxiv.org/abs/2107.03374.\\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten,\\nA.Yang,A.Fan,etal. Thellama3herdofmodels. arXivpreprintarXiv:2407.21783,2024.\\nY.Dubois,B.Galambosi,P.Liang,andT.B.Hashimoto. Length-controlledalpacaeval: Asimple\\nwaytodebiasautomaticevaluators. arXivpreprintarXiv:2404.04475,2024.\\nX. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like\\ntree-search can guide large language model decoding and training, 2024. URL https:\\n//arxiv.org/abs/2309.17179.\\nL.Gao,J.Schulman,andJ.Hilton. Scalinglawsforrewardmodeloveroptimization,2022. URL\\nhttps://arxiv.org/abs/2210.10760.\\nA.P.Gema, J.O.J.Leang, G.Hong, A.Devoto, A.C.M.Mancino, R.Saxena, X.He, Y.Zhao,\\nX. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and\\nP.Minervini. Arewedonewithmmlu? CoRR,abs/2406.04127,2024. URLhttps://doi.or\\ng/10.48550/arXiv.2406.04127.\\nGoogle. Ournext-generationmodel: Gemini1.5,2024. URLhttps://blog.google/techno\\nlogy/ai/google-gemini-next-generation-model-february-2024.\\nY. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi-\\nnese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint\\narXiv:2411.07140,2024.\\nD.Hendrycks,C.Burns,S.Basart,A.Zou,M.Mazeika,D.Song,andJ.Steinhardt. Measuring\\nmassivemultitasklanguageunderstanding. arXivpreprintarXiv:2009.03300,2020.\\nY.Huang,Y.Bai,Z.Zhu,J.Zhang,J.Zhang,T.Su,J.Liu,C.Lv,Y.Zhang,J.Lei,etal. C-Eval: A\\nmulti-levelmulti-disciplinechineseevaluationsuiteforfoundationmodels. arXivpreprint\\narXiv:2305.08322,2023.\\nN.Jain,K.Han,A.Gu,W.Li,F.Yan,T.Zhang,S.Wang,A.Solar-Lezama,K.Sen,andI.Stoica.\\nLivecodebench: Holisticandcontaminationfreeevaluationoflargelanguagemodelsforcode.\\nCoRR,abs/2403.07974,2024. URLhttps://doi.org/10.48550/arXiv.2403.07974.',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 17,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'S.Krishna,K.Krishna,A.Mohananey,S.Schwarcz,A.Stambler,S.Upadhyay,andM.Faruqui.\\nFact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR,\\nabs/2409.12941,2024. doi: 10.48550/ARXIV.2409.12941. URLhttps://doi.org/10.485\\n50/arXiv.2409.12941.\\nA.Kumar,V.Zhuang,R.Agarwal,Y.Su,J.D.Co-Reyes,A.Singh,K.Baumli,S.Iqbal,C.Bishop,\\nR.Roelofs,etal. Traininglanguagemodelstoself-correctviareinforcementlearning. arXiv\\npreprintarXiv:2409.12917,2024.\\nH.Li,Y.Zhang,F.Koto,Y.Yang,H.Zhao,Y.Gong,N.Duan,andT.Baldwin. CMMLU:Measur-\\ningmassivemultitasklanguageunderstandinginChinese. arXivpreprintarXiv:2306.09212,\\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From\\ncrowdsourceddatatohigh-qualitybenchmarks: Arena-hardandbenchbuilderpipeline. arXiv\\npreprintarXiv:2406.11939,2024.\\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,\\nI.Sutskever,andK.Cobbe. Letâ€™sverifystepbystep. arXivpreprintarXiv:2305.20050,2023.\\nB.Y.Lin. ZeroEval: AUnifiedFrameworkforEvaluatingLanguageModels,July2024. URL\\nhttps://github.com/WildEval/ZeroEval.\\nMAA. American invitational mathematics examination - aime. In American Invitational\\nMathematics Examination - AIME 2024, February 2024. URL https://maa.org/math\\n-competitions/american-invitational-mathematics-examination-aime.\\nOpenAI. HelloGPT-4o,2024a. URLhttps://openai.com/index/hello-gpt-4o/.\\nOpenAI. Learningtoreasonwithllms,2024b. URLhttps://openai.com/index/learnin\\ng-to-reason-with-llms/.\\nOpenAI. IntroducingSimpleQA,2024c. URLhttps://openai.com/index/introducing\\n-simpleqa/.\\nOpenAI. Introducing SWE-bench verified weâ€™re releasing a human-validated subset of swe-\\nbenchthatmore, 2024d. URLhttps://openai.com/index/introducing-swe-bench\\n-verified/.\\nQwen. Qwq: Reflectdeeplyontheboundariesoftheunknown,2024a. URLhttps://qwenlm\\n.github.io/blog/qwq-32b-preview/.\\nQwen. Qwen2.5: Apartyoffoundationmodels,2024b. URLhttps://qwenlm.github.io/b\\nlog/qwen2.5.\\nD.Rein,B.L.Hou,A.C.Stickland,J.Petty,R.Y.Pang,J.Dirani,J.Michael,andS.R.Bowman.\\nGPQA:Agraduate-levelgoogle-proofq&abenchmark. arXivpreprintarXiv:2311.12022,2023.\\nZ.Shao,P.Wang,Q.Zhu,R.Xu,J.Song,M.Zhang,Y.Li,Y.Wu,andD.Guo. Deepseekmath:\\nPushing the limits of mathematical reasoning in open language models. arXiv preprint\\narXiv:2402.03300,2024.\\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,\\nD.Kumaran,T.Graepel,T.P.Lillicrap,K.Simonyan,andD.Hassabis. Masteringchessand\\nshogibyself-playwithageneralreinforcementlearningalgorithm. CoRR,abs/1712.01815,\\n2017a. URLhttp://arxiv.org/abs/1712.01815.',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 18,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'D.Silver,J.Schrittwieser,K.Simonyan,I.Antonoglou,A.Huang,A.Guez,T.Hubert,L.Baker,\\nM.Lai,A.Bolton,Y.Chen,T.P.Lillicrap,F.Hui,L.Sifre,G.vandenDriessche,T.Graepel,and\\nD.Hassabis. Masteringthegameofgowithouthumanknowledge. Nat.,550(7676):354â€“359,\\n2017b. doi: 10.1038/NATURE24270. URLhttps://doi.org/10.1038/nature24270.\\nC. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more\\neffectivethanscalingmodelparameters,2024. URLhttps://arxiv.org/abs/2408.033\\nT. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human\\ndemonstrations. Nature,2024. doi: 10.1038/s41586-023-06747-5.\\nJ. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and\\nI.Higgins. Solvingmathwordproblemswithprocess-andoutcome-basedfeedback. arXiv\\npreprintarXiv:2211.14275,2022.\\nP.Wang,L.Li,Z.Shao,R.Xu,D.Dai,Y.Li,D.Chen,Y.Wu,andZ.Sui. Math-shepherd: Alabel-\\nfreestep-by-stepverifierforllmsinmathematicalreasoning. arXivpreprintarXiv:2312.08935,\\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou.\\nSelf-consistency improves chain of thought reasoning in language models. arXiv preprint\\narXiv:2203.11171,2022.\\nY.Wang,X.Ma,G.Zhang,Y.Ni,A.Chandra,S.Guo,W.Ren,A.Arulraj,X.He,Z.Jiang,T.Li,\\nM. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and\\nchallenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024.\\nURLhttps://doi.org/10.48550/arXiv.2406.01574.\\nC. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software\\nengineeringagents. arXivpreprint,2024.\\nH.Xin,Z.Z.Ren,J.Song,Z.Shao,W.Zhao,H.Wang,B.Liu,L.Zhang,X.Lu,Q.Du,W.Gao,\\nQ.Zhu,D.Yang,Z.Gou,Z.F.Wu,F.Luo,andC.Ruan. Deepseek-prover-v1.5: Harnessing\\nproofassistantfeedbackforreinforcementlearningandmonte-carlotreesearch,2024. URL\\nhttps://arxiv.org/abs/2408.08152.\\nJ.Zhou,T.Lu,S.Mishra,S.Brahma,S.Basu,Y.Luan,D.Zhou,andL.Hou. Instruction-following\\nevaluationforlargelanguagemodels. arXivpreprintarXiv:2311.07911,2023.',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 19,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'A. Contributions and Acknowledgments\\nCoreContributors HuiLi\\nDayaGuo JianzhongGuo\\nDejianYang JiashiLi\\nHaoweiZhang JingchangChen\\nJunxiaoSong JingyangYuan\\nRuoyuZhang JinhaoTu\\nRunxinXu JunjieQiu\\nQihaoZhu JunlongLi\\nShirongMa J.L.Cai\\nPeiyiWang JiaqiNi\\nXiaoBi JianLiang\\nXiaokangZhang JinChen\\nXingkaiYu KaiDong\\nYuWu KaiHu*\\nZ.F.Wu KaichaoYou\\nZhibinGou KaigeGao\\nZhihongShao KangGuan\\nZhuoshuLi KexinHuang\\nZiyiGao KuaiYu\\nLecongZhang\\nContributors\\nBingxuanWang\\nMingchuanZhang\\nMinghuaZhang\\nChenggangZhao\\nMinghuiTang\\nChengqiDeng\\nMiaojunWang\\nPanpanHuang\\nQianchengWang\\nGuantingChen\\nRuisongZhang\\nHonghuiDing',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 20,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'RuyiChen Y.X.Wei\\nShanghaoLu YangZhang\\nShangyanZhou YanhongXu\\nShanhuangChen YaoLi\\nShengfengYe YaoZhao\\nShiyuWang YaofengSun\\nShuipingYu YaohuiWang\\nShunfengZhou YiYu\\nShutingPan YichaoZhang\\nS.S.Li YifanShi\\nShuangZhou YiliangXiong\\nShaoqingWu YingHe\\nShengfengYe YishiPiao\\nTaoYun YisongWang\\nTianPei YixuanTan\\nTianyuSun YiyangMa*\\nT.Wang YiyuanLiu\\nWangdingZeng YongqiangGuo\\nWenLiu YuanOu\\nWenfengLiang YuduanWang\\nWenjunGao YueGong\\nWenqinYu* YuhengZou\\nWentaoZhang YujiaHe\\nW.L.Xiao YunfanXiong\\nWeiAn YuxiangLuo\\nXiaodongLiu YuxiangYou\\nXiaohanWang YuxuanLiu\\nXiaokangChen YuyangZhou\\nXiaotaoNie Y.X.Zhu\\nXinCheng YanpingHuang\\nXinLiu YaohuiLi\\nXinXie YiZheng\\nXingchaoLiu YuchenZhu\\nXinyuYang YunxianMa\\nXinyuanLi YingTang\\nXuechengSu YukunZha\\nXuhengLin YutingYan\\nX.Q.Li Z.Z.Ren\\nXiangyueJin ZehuiRen\\nXiaojinShen ZhangliSha\\nXiaoshaChen ZheFu\\nXiaowenSun ZheanXu\\nXiaoxiangWang ZhendaXie\\nXinnanSong ZhengyanZhang\\nXinyiZhou ZhewenHao\\nXianzuWang ZhichengMa\\nXinxiaShan ZhigangYan\\nY.K.Li ZhiyuWu\\nY.Q.Wang ZihuiGu',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 21,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}},\n",
       " {'page_content': 'ZijiaZhu ZhenHuang\\nZijunLiu* ZhipengXu\\nZilinLi ZhongyuZhang\\nZiweiXie ZhenZhang\\nWithineachrole,authorsarelistedalphabeticallybythefirstname. Namesmarkedwith*\\ndenoteindividualswhohavedepartedfromourteam.',\n",
       "  'metadata': {'source': 'DeepSeek-R1',\n",
       "   'page_number': 22,\n",
       "   'file_path': '../data/DeepSeek-R1.pdf'}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1b04f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking.py\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "\n",
    "def get_text_splitter(strategy=\"recursive\", chunk_size=800, chunk_overlap=150, model_name=\"gpt-4o\"):\n",
    "    if strategy == \"token\":\n",
    "        return TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, model_name=model_name)\n",
    "    # default: character recursive (works well)\n",
    "    return RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# usage\n",
    "splitter = get_text_splitter(strategy=\"recursive\", chunk_size=800, chunk_overlap=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa230ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = splitter.split_text(text_documents[0][\"page_content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b9ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9783e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def gemini_embedding_batch(texts, batch_size=2):\n",
    "    result_emb = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        result_emb.extend(embeddings.embed_documents(batch))\n",
    "        time.sleep(2)  # to avoid rate limits\n",
    "    return result_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77720615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make chunks of dataset \n",
    "\n",
    "\n",
    "def chunking_doc(documents):\n",
    "    chunking_dataset = []\n",
    "    for document in documents:\n",
    "        texts = splitter.split_text(document[\"page_content\"])\n",
    "        for text in texts:\n",
    "            chunking_dataset.append({\n",
    "                \"page_content\": text,\n",
    "                \"metadata\": document[\"metadata\"]\n",
    "            })\n",
    "    return chunking_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "73dca3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4a298b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunking_text_document = chunking_doc(text_documents)\n",
    "len(chunking_text_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d1f98ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_and_store(docs):\n",
    "    # chunking \n",
    "    texts = [doc['page_content'] for doc in docs]\n",
    "    metadatas = [doc['metadata'] for doc in docs]\n",
    "    embs = gemini_embedding_batch(texts)\n",
    "\n",
    "    \n",
    "    ids = [f\"doc_{i}\" for i in range(len(docs))]\n",
    "    \n",
    "    collection.add(\n",
    "        documents=texts,\n",
    "        metadatas=metadatas,\n",
    "        embeddings=embs,\n",
    "        ids=ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "00ea3e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [02:40<00:00,  2.72s/it]\n"
     ]
    }
   ],
   "source": [
    "embed_and_store(chunking_text_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed3de1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "db = Chroma(collection_name=\"rag_collection\", client=client, embedding_function=embeddings)\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5})\n",
    "\n",
    "mmr_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":5, \"fetch_k\":10, \"lambda_mult\":0.5})\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "cross_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")  # small and fast-ish\n",
    "\n",
    "def rerank_with_cross_encoder(query, candidates):\n",
    "    # candidates: list of Document-like dicts with 'page_content'\n",
    "    pairs = [[query, c.page_content] for c in candidates]\n",
    "    scores = cross_model.predict(pairs)\n",
    "    ranked = sorted(zip(scores, candidates), key=lambda x: x[0], reverse=True)\n",
    "    return [c for s, c in ranked]\n",
    "\n",
    "llm = model\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Use the retrieved documents to answer the user query.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type=\"stuff\",  # or 'map_reduce' or 'refine'\n",
    "                                       retriever=retriever,\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={ \"prompt\": prompt })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f57a29ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query):\n",
    "    # Step A: get top candidates from vectordb (fetch_k large)\n",
    "    docs = db._collection.query(query_embeddings=embeddings.embed_query(query), n_results=20)[\"documents\"][0]\n",
    "    # convert to Document objects if required; this access is implementation-dependent in Chroma.\n",
    "    from langchain_core.documents import Document\n",
    "    candidate_docs = [Document(page_content=d, metadata={}) for d in docs]\n",
    "    # Step B: rerank with cross-encoder\n",
    "    reranked = rerank_with_cross_encoder(query, candidate_docs)[:5]\n",
    "    # Step C: merge into context\n",
    "    combined_context = \"\\n\\n---\\n\\n\".join([d.page_content for d in reranked])\n",
    "    # Step D: call LLM\n",
    "    response = llm.invoke(prompt.format(context=combined_context, question=query))\n",
    "    return response, reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9f9ee2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans, sources = answer_query(\"list step for transformer model deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "05a0106e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Action for you: Try to implement a tiny Transformer from scratch in\\nPyTorch or TensorFlow.\\n2. Pretraining of LLMs\\nLLMs like GPT, LLaMA, Falcon, Mistral are trained in two main stages.\\nPretraining Objective\\nUsually causal language modeling (CLM) â†’ predict next token.\\nOn massive datasets (hundreds of billions of tokens).\\nTraining Pipeline\\nTokenization (BPE, SentencePiece, WordPiece)\\nDatasets (Common Crawl, books, code, Wikipedia, etc.)\\nLoss Function: Cross-Entropy\\nOptimization: AdamW, learning rate warm-up, cosine decay\\nMixed Precision Training (FP16/BF16)\\nGradient Accumulation, Gradient Clipping\\nDistributed Training: Data Parallelism, Model Parallelism, Pipeline\\nParallelism\\nDeep dive topics: scaling laws, compute-data tradeoff, Chinchilla optimal\\n3. Fine-Tuning LLMs'),\n",
       " Document(metadata={}, page_content='Stage 5: Advanced Architectures & Research\\nGoal: Explore cutting-edge improvements.\\nConcepts to Learn\\nMixture of Experts (MoE) models (Switch Transformer, Mixtral)\\nLong-context attention (RoPE, ALiBi, FlashAttention)\\nRetrieval-Augmented Generation (RAG)\\nEvaluation frameworks: MMLU, HELM, MT-Bench\\nKey Resources\\nSwitch Transformers (Fedus et al., 2021)\\nFlashAttention (Dao et al., 2022)\\nRetrieval-Augmented Generation (Lewis et al., 2020)\\nStanford HELM benchmark\\nCoding Projects\\nAdd retrieval (RAG) to an open-source LLM using FAISS/Chroma DB.\\nExperiment with long-context models (Mistral-Long, LLaMA-Long).\\nFine-tune a reward model for preferences (mini-RLHF experiment).\\nStage 6: Putting It All Together\\nGoal: Build an end-to-end mini-LLM project.'),\n",
       " Document(metadata={}, page_content='Would you like me to make a step-by-step curriculum (with resources,\\npapers, and small coding projects at each stage), so you can follow along like\\na structured learning plan?\\nPerfect â€” letâ€™s build you a step-by-step curriculum for learning how large\\nlanguage models (LLMs) work, how to train/fine-tune them, and how to make\\ninference efficient. Iâ€™ll mix theory, papers, and coding projects so you get\\nboth intuition and hands-on skills.\\nStep-by-Step Curriculum for LLMs\\nStage 1: Core Foundations\\nGoal: Build intuition about neural nets, attention, and Transformers.\\nConcepts to Learn\\nNeural nets basics (MLPs, backpropagation, optimization)\\nWord embeddings & tokenization\\nRNNs â†’ LSTMs â†’ Attention mechanism\\nTransformer architecture\\nKey Resources\\nThe Illustrated Transformer (visual explanation)'),\n",
       " Document(metadata={}, page_content='Word embeddings & tokenization\\nRNNs â†’ LSTMs â†’ Attention mechanism\\nTransformer architecture\\nKey Resources\\nThe Illustrated Transformer (visual explanation)\\nAttention Is All You Need (2017) â€“ original Transformer paper\\nStanford CS324: Large Language Models (lectures on YouTube)\\nCoding Projects\\nImplement a tiny Transformer (2â€“3 layers) in PyTorch.'),\n",
       " Document(metadata={}, page_content='embeddings â†’ transformer layers â†’ quantized output)?\\nexplain lora in simple word\\nAbsolutely â€” letâ€™s explain LoRA (Low-Rank Adaptation) in very simple\\nterms, step by step.\\nWhat Is LoRA?\\nLoRA stands for Low-Rank Adaptation â€”\\nitâ€™s a method to fine-tune large language models (LLMs) without changing\\nmost of their weights.\\nIt was introduced to make fine-tuning big models cheap and efficient .\\nImagine This Scenario\\nYou have a huge model like LLaMA 7B (7 billion parameters).\\nFine-tuning all its parameters would:\\nNeed lots of GPU memory\\nTake days or weeks\\nRisk forgetting what it already knows (catastrophic forgetting)\\nSo instead of updating all weights, LoRA says:\\nâ€œLetâ€™s keep the original model frozen and just learn some small extra\\nHow It Works (Simplified)')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fe8d9afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents, there isn't a direct step-by-step guide for Transformer model deployment. However, we can infer some general stages and concepts that would be involved in deploying such a model.\n",
      "\n",
      "Here's a breakdown of potential steps, drawing from the information about LLM training and advanced architectures:\n",
      "\n",
      "**Inferred Steps for Transformer Model Deployment:**\n",
      "\n",
      "1.  **Model Training/Fine-tuning (Pre-deployment Stage):**\n",
      "    *   **Pretraining:** This is the initial stage where the model learns general language understanding on massive datasets. The documents mention \"causal language modeling (CLM)\" as a common pretraining objective.\n",
      "    *   **Fine-tuning:** After pretraining, the model is often fine-tuned for specific tasks or domains. This involves using smaller, task-specific datasets and techniques like LoRA (Low-Rank Adaptation) for efficient adaptation.\n",
      "\n",
      "2.  **Model Optimization for Inference:**\n",
      "    *   **Quantization:** This is a technique to reduce the precision of the model's weights (e.g., from FP16/BF16 to INT8). The mention of \"quantized output\" in one of the snippets suggests this is a relevant concept. Quantization significantly reduces model size and speeds up inference.\n",
      "    *   **Efficient Attention Mechanisms:** For models dealing with long contexts, deploying them efficiently might involve using optimized attention mechanisms like FlashAttention, as mentioned in the \"Advanced Architectures\" section.\n",
      "\n",
      "3.  **Inference Pipeline Setup:**\n",
      "    *   **Tokenization:** The input text needs to be tokenized using the same method (e.g., BPE, SentencePiece, WordPiece) used during training.\n",
      "    *   **Model Loading:** The optimized model weights need to be loaded into memory.\n",
      "    *   **Forward Pass:** The tokenized input is fed through the Transformer model to generate predictions.\n",
      "    *   **Decoding/Output Generation:** The model's output (often probabilities for the next token) is converted back into human-readable text using a decoding strategy (e.g., greedy decoding, beam search).\n",
      "\n",
      "4.  **Deployment Infrastructure:**\n",
      "    *   **Hardware:** Choosing appropriate hardware (GPUs, TPUs) for efficient inference.\n",
      "    *   **Serving Frameworks:** Utilizing frameworks designed for serving machine learning models (e.g., TensorFlow Serving, TorchServe, custom solutions).\n",
      "    *   **Scalability and Load Balancing:** Ensuring the deployment can handle the expected traffic and requests.\n",
      "\n",
      "5.  **Monitoring and Maintenance:**\n",
      "    *   **Performance Monitoring:** Tracking inference speed, latency, and resource utilization.\n",
      "    *   **Model Updates:** Deploying new versions of the model as they are retrained or fine-tuned.\n",
      "\n",
      "**Key Concepts from Documents Relevant to Deployment:**\n",
      "\n",
      "*   **Mixed Precision Training (FP16/BF16):** While mentioned for training, using lower precision (like FP16 or even INT8 through quantization) is crucial for efficient inference.\n",
      "*   **Tokenization:** Essential for both training and inference to convert text to numerical representations.\n",
      "*   **Advanced Architectures (FlashAttention):** Techniques that improve computational efficiency can be leveraged for faster inference.\n",
      "*   **LoRA:** While primarily for fine-tuning, the concept of parameter-efficient adaptation can lead to smaller models that are easier to deploy.\n",
      "\n",
      "It's important to note that the provided snippets focus heavily on the training and architectural aspects of LLMs, with less emphasis on the operational side of deployment. A complete deployment guide would involve much more detail on infrastructure, API design, and continuous integration/continuous deployment (CI/CD) pipelines.\n"
     ]
    }
   ],
   "source": [
    "print(ans.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9792a409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1861c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f0e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb7b75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a455933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4fe033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7ddc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
